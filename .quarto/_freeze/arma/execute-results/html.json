{
  "hash": "d33f2435d41cbe4af9569525c830f3f7",
  "result": {
    "markdown": "---\ntitle: \"ARMA/ARIMA/SARIMA Models\"\n---\n\n::: {.cell}\n\n:::\n\n\nIn this tab, I will focus on using these time series models, such as ARIMA (Auto-regressive integrated moving average) and SARIMA (Seasonal Auto-regressive Integrated Moving Average) to perform some forecasting on the crop production data\n\n## USA Wheat Yield (ARIMA Model)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncp = read.csv(\"./HW3/Monthly_Data.csv\")\n\nwheat_usa = cp[cp$Crop==\"Wheat\",]\ncolnames(wheat_usa)[4] = c(\"Yield\")\nwheat_usa$Yield = na.approx(wheat_usa$Yield)\n\ncorn_usa = cp[cp$Crop==\"Corn\",]\ncolnames(corn_usa)[4] = c(\"Yield\")\ncorn_usa$Yield = na.approx(corn_usa$Yield)\n\nstart_date = c(2002, 8)\nwheat_ts = ts(wheat_usa$Yield, start=start_date, frequency=12)\ncorn_ts = ts(corn_usa$Yield, start=start_date, frequency=12)\n```\n:::\n\n\nFrom the exploratory data analysis part, there is some important information on the wheat yield data so it will be used in the ARIMA model experiment.\n\nAt the [step](./eda.html) where I plot the ACF and PACF plots, and where I performed the Augmented Dickey-Fuller Test, it is shown that the original time series is stationary. But to get rid of any of the seasonality, I will try doing one differencing on the time series.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiff_ts = diff(wheat_ts)\ntseries::adf.test(diff_ts)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tAugmented Dickey-Fuller Test\n\ndata:  diff_ts\nDickey-Fuller = -6.1309, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n```\n:::\n:::\n\n\nAgain, the Augmented Dickey-Fuller Test (ADF test) can confirm that the time series after differencing is clearly stationary with p-value=0.01.\n\n#### \n\n### ACF & PACF plots\n\nFirst, plot the ACF and PACF plots and decide the order of the AR(p) and MA(q) processes in the ARIMA(p,d,q) model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggAcf(diff_ts) + ggtitle(\"ACF of Monthly Wheat Production in USA\")\n```\n\n::: {.cell-output-display}\n![](arma_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggPacf(diff_ts) + ggtitle(\"PACF of Monthly Wheat Production in USA\")\n```\n\n::: {.cell-output-display}\n![](arma_files/figure-html/unnamed-chunk-4-2.png){width=672}\n:::\n:::\n\n\nThen, from the ACF and PACF graphs, I will choose a set of values for the parameters: p=(0:1) and q=(0:4). Since I did one differencing to the time series, d=1 in this case.\n\n#### \n\n### Fit ARIMA with choices of parameters\n\nThen, I will fit my choice of ARIMA(p,d,q) to the data, and select the model with lowest AIC, BIC.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nscores = matrix(rep(0, 5*10), nrow=10, ncol=5)\n\nd=1\nfor (p in 0:1){\n  for (q in 0:4){\n    model = Arima(wheat_ts, order=c(p,d,q), include.drift=TRUE)\n    scores[p*5+q+1,] = c(p,q,model$aic, model$bic, model$aicc)\n  }\n}\n\nscores = as.data.frame(scores)\ncolnames(scores) = c(\"p\",\"q\",\"AIC\",\"BIC\",\"AICC\")\nknitr::kable(scores, format=\"pipe\", padding=30, digits=2)\n```\n\n::: {.cell-output-display}\n|                              p|                              q|                                  AIC|                                  BIC|                                 AICC|\n|------------------------------:|------------------------------:|------------------------------------:|------------------------------------:|------------------------------------:|\n|                              0|                              0|                              -679.11|                              -672.11|                              -679.06|\n|                              0|                              1|                              -686.67|                              -676.18|                              -686.57|\n|                              0|                              2|                              -686.04|                              -672.05|                              -685.87|\n|                              0|                              3|                              -685.74|                              -668.25|                              -685.48|\n|                              0|                              4|                              -683.87|                              -662.89|                              -683.51|\n|                              1|                              0|                              -687.41|                              -676.92|                              -687.31|\n|                              1|                              1|                              -685.41|                              -671.42|                              -685.25|\n|                              1|                              2|                              -685.48|                              -668.00|                              -685.23|\n|                              1|                              3|                              -683.87|                              -662.89|                              -683.52|\n|                              1|                              4|                              -681.87|                              -657.39|                              -681.39|\n:::\n:::\n\n\nFrom the results of the AIC and BIC scores, I can determine that the model with the lowest AIC, BIC has parameters p=1, q=0. Therefore, the model will be ARIMA(1,1,0).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit10 = Arima(wheat_ts, order=c(1,1,0), include.drift=TRUE)\nsummary(fit10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSeries: wheat_ts \nARIMA(1,1,0) with drift \n\nCoefficients:\n         ar1   drift\n      0.2029  0.0031\ns.e.  0.0625  0.0047\n\nsigma^2 = 0.003442:  log likelihood = 346.71\nAIC=-687.41   AICc=-687.31   BIC=-676.92\n\nTraining set error measures:\n                       ME       RMSE       MAE          MPE     MAPE      MASE\nTraining set 8.570452e-06 0.05830938 0.0328281 -0.006918382 1.102147 0.1870143\n                     ACF1\nTraining set 0.0004080243\n```\n:::\n:::\n\n\nThen, the equation of this model is:\n\n$$\n(X_t-0.203X_{t-1}=w_t)\n$$\n\n#### \n\n### Model Diagnostic\n\nThis part contains some technique overview and diagnostic about this model. I will use some plots to describe about the residuals.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_output <- capture.output(sarima(wheat_ts,1,1,0))\n```\n\n::: {.cell-output-display}\n![](arma_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncheckresiduals(fit10)\n```\n\n::: {.cell-output-display}\n![](arma_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tLjung-Box test\n\ndata:  Residuals from ARIMA(1,1,0) with drift\nQ* = 27.624, df = 23, p-value = 0.2303\n\nModel df: 1.   Total lags used: 24\n```\n:::\n:::\n\n\nHere is the result of checking residual of the ARIMA(1,1,0) model. From the Ljung-Box test, the residuals should be independent according to a p-value larger than 0.05, and this means the model fits the time series well. From the Q-Q plot we can also see basically the residuals form a normal distribution.\n\n#### \n\n### Compare with `auto.arima()`\n\nTo test if my model agrees with the best model chosen by the auto process, I will try to use a \\`auto.arima()\\` function for testing.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nauto.arima(wheat_ts, seasonal=FALSE, ic = \"aic\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSeries: wheat_ts \nARIMA(0,1,1) \n\nCoefficients:\n         ma1\n      0.1876\ns.e.  0.0577\n\nsigma^2 = 0.003445:  log likelihood = 346.1\nAIC=-688.2   AICc=-688.15   BIC=-681.2\n```\n:::\n:::\n\n\nApparently the result is different from my chosen model, and the AIC and BIC of the auto result is also larger. This may be because the \\`auto.arima\\` function does not search through enough possible parameters to find the model with the best statistics.\n\n#### \n\n### Forecasting\n\nNow having my optimal model I can forecast using it with a confidence band. I will then plot the forecasts.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred = forecast(fit10, 24)\npred_ts = ts(pred$mean, start=c(2023,1), frequency=12)\nts.plot(wheat_ts, pred_ts, gpars = list(col = c(\"black\", \"red\")))\n```\n\n::: {.cell-output-display}\n![](arma_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nThe forecasting result of 2 years show that there will be a steady increase in the yield of wheat, which follows the general trend of the data, however the actual yield might not look as simple.\n\n#### \n\n### Compare with Benchmark\n\nHere, I will compare my optimal ARIMA model with other benchmark methods such as mean, naive, snaive and drift, using MAE and RMSE.\n\nI plan to do this with training and testing sets, where I can train the different models and methods on the training set, and the RMSE of the models on the testing set can be reported and compared.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_ts = ts(wheat_usa$Yield[1:196], start=c(2002, 8), frequency = 12)\ntest_ts = ts(wheat_usa$Yield[197:245], start=c(2018, 12), frequency = 12)\n\narima_train = Arima(train_ts, order=c(1,1,0), include.drift = TRUE)\narima_pred = forecast(arima_train,49)\narima_res = arima_pred$mean-test_ts\n\nmean_pred = meanf(train_ts, h=49)\nmean_res = mean_pred$mean-test_ts\nnaive_pred = naive(train_ts, h=49)\nnaive_res = naive_pred$mean-test_ts\nsnaive_pred = snaive(train_ts, h=49)\nsnaive_res = snaive_pred$mean-test_ts\ndrift_pred = rwf(train_ts, h=49, drift=TRUE)\ndrift_res = drift_pred$mean-test_ts\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\narima_rmse = sqrt(mean(arima_res**2))\nmean_rmse = sqrt(mean(mean_res**2))\nnaive_rmse = sqrt(mean(naive_res**2))\nsnaive_rmse = sqrt(mean(snaive_res**2))\ndrift_rmse = sqrt(mean(drift_res**2))\n\ncomparison = data.frame(\n  methods=c(\"ARIMA\", \"Mean\", \"Naive\", \"Snaive\", \"Drift\"),\n  RMSE=c(arima_rmse, mean_rmse, naive_rmse, snaive_rmse, drift_rmse)\n)\n\nknitr::kable(comparison, format=\"pipe\", padding=30, digits=2)\n```\n\n::: {.cell-output-display}\n|methods                              |                              RMSE|\n|:------------------------------------|---------------------------------:|\n|ARIMA                                |                              0.20|\n|Mean                                 |                              0.34|\n|Naive                                |                              0.15|\n|Snaive                               |                              0.18|\n|Drift                                |                              0.20|\n:::\n:::\n\n\nBased on the result of the comparison, the RMSE of the optimal ARIMA model is much less than the mean method, the same as the Drift method, but a little larger than Naive and Snaive method. The numbers here show that the ARIMA model has a decent performance on the wheat production time series.\n\nThen, I will compare the forcasts results using a plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(train_ts) +\n  autolayer(meanf(train_ts, h=49),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(train_ts, h=49),\n            series=\"Na誰ve\", PI=FALSE) +\n  autolayer(snaive(train_ts, h=49),\n            series=\"SNa誰ve\", PI=FALSE)+\n  autolayer(rwf(train_ts, h=49, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(arima_train, 49), \n            series=\"ARIMA\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n```\n\n::: {.cell-output-display}\n![](arma_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nIt seems that the ARIMA(1,1,0) model gives a forecast that is simple and basically follows the previous trend, and is basically the same as the forecast of the drift method. Except for mean method which is far off, the other methods give relatively reliable forecasts.\n\n#### \n\n## USA Wheat Price (SARIMA Model)\n\nAt this step I will move on to another focus of this project, the price of the wheat in the United States. First, look at the plot and the decompose of the wheat price time series:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nusa_wheat = read.csv(\"./HW4/USA_Wheat.csv\")\nusa_wheat$Price = na.approx(usa_wheat$Price)\n\nstart_date = c(2002, 8)\nprice_ts = ts(usa_wheat$Price, start=start_date, frequency=12)\n\nautoplot(price_ts)+xlab(\"Date\")+ylab(\"Price\")\n```\n\n::: {.cell-output-display}\n![](arma_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n\n```{.r .cell-code}\nautoplot(decompose(price_ts, \"additive\"))\n```\n\n::: {.cell-output-display}\n![](arma_files/figure-html/unnamed-chunk-14-2.png){width=672}\n:::\n:::\n\n\nIt looks like the trend of the wheat price is not clear through out the years, and the seasonality is obvious.\n\n#### \n\n### Check for stationarity\n\nTo check the seasonal component and the stationarity, I will use a ACF graph and a test:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggAcf(price_ts,200)\n```\n\n::: {.cell-output-display}\n![](arma_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n\n```{.r .cell-code}\ntseries::adf.test(price_ts)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tAugmented Dickey-Fuller Test\n\ndata:  price_ts\nDickey-Fuller = -2.8178, Lag order = 6, p-value = 0.2317\nalternative hypothesis: stationary\n```\n:::\n:::\n\n\nIt looks like the time series may have seasonal affect, and is definitely not stationary. Therefore, I will do a seasonal difference in addition to a general differencing to the time series.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprice_ts %>% diff(lag=12) %>% diff() %>% ggtsdisplay()\n```\n\n::: {.cell-output-display}\n![](arma_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n#### \n\n### Look for the Best Model\n\nBased on the ACF and the PACF plots, it can be inferred that d=1, D=1, p=\\[0,1,2\\], q=\\[0,1,2\\], P=\\[1,2\\], Q=\\[1,2\\]. Then, I will fit my choice of SARIMA(p,d,q,P,D,Q) to the data, and select the model with lowest AIC, BIC.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nscores = matrix(rep(0, 9*36), nrow=36, ncol=9)\n\nd=1\nD=1\ni=1\nfor (p in 0:2){\n  for (q in 0:2){\n    for (P in 1:2){\n      for (Q in 1:2) {\n        model = sarima(price_ts,p,d,q,P,D,Q,S=12,details=FALSE)\n        scores[i,] = c(p,d,q,P,D,Q,model$AIC,model$BIC,model$AICc)\n        i = i+1\n      }\n\n    }\n  }\n}\n\nscores = as.data.frame(scores)\ncolnames(scores) = c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICC\")\nknitr::kable(scores, format=\"pipe\", padding=30, digits=2)\n```\n\n::: {.cell-output-display}\n|                              p|                              d|                              q|                              P|                              D|                              Q|                               AIC|                               BIC|                              AICC|\n|------------------------------:|------------------------------:|------------------------------:|------------------------------:|------------------------------:|------------------------------:|---------------------------------:|---------------------------------:|---------------------------------:|\n|                              0|                              1|                              0|                              1|                              1|                              1|                              0.82|                              0.87|                              0.82|\n|                              0|                              1|                              0|                              1|                              1|                              2|                              0.83|                              0.89|                              0.83|\n|                              0|                              1|                              0|                              2|                              1|                              1|                              0.83|                              0.89|                              0.83|\n|                              0|                              1|                              0|                              2|                              1|                              2|                              0.83|                              0.90|                              0.83|\n|                              0|                              1|                              1|                              1|                              1|                              1|                              0.69|                              0.75|                              0.69|\n|                              0|                              1|                              1|                              1|                              1|                              2|                              0.69|                              0.76|                              0.69|\n|                              0|                              1|                              1|                              2|                              1|                              1|                              0.69|                              0.76|                              0.69|\n|                              0|                              1|                              1|                              2|                              1|                              2|                              0.69|                              0.78|                              0.70|\n|                              0|                              1|                              2|                              1|                              1|                              1|                              0.68|                              0.76|                              0.68|\n|                              0|                              1|                              2|                              1|                              1|                              2|                              0.69|                              0.77|                              0.69|\n|                              0|                              1|                              2|                              2|                              1|                              1|                              0.69|                              0.78|                              0.69|\n|                              0|                              1|                              2|                              2|                              1|                              2|                              0.69|                              0.79|                              0.69|\n|                              1|                              1|                              0|                              1|                              1|                              1|                              0.69|                              0.75|                              0.69|\n|                              1|                              1|                              0|                              1|                              1|                              2|                              0.70|                              0.77|                              0.70|\n|                              1|                              1|                              0|                              2|                              1|                              1|                              0.69|                              0.77|                              0.70|\n|                              1|                              1|                              0|                              2|                              1|                              2|                              0.70|                              0.79|                              0.70|\n|                              1|                              1|                              1|                              1|                              1|                              1|                              0.69|                              0.76|                              0.69|\n|                              1|                              1|                              1|                              1|                              1|                              2|                              0.69|                              0.78|                              0.70|\n|                              1|                              1|                              1|                              2|                              1|                              1|                              0.69|                              0.78|                              0.69|\n|                              1|                              1|                              1|                              2|                              1|                              2|                              0.70|                              0.80|                              0.70|\n|                              1|                              1|                              2|                              1|                              1|                              1|                              0.69|                              0.78|                              0.69|\n|                              1|                              1|                              2|                              1|                              1|                              2|                              0.69|                              0.79|                              0.69|\n|                              1|                              1|                              2|                              2|                              1|                              1|                              0.69|                              0.80|                              0.69|\n|                              1|                              1|                              2|                              2|                              1|                              2|                              0.70|                              0.81|                              0.70|\n|                              2|                              1|                              0|                              1|                              1|                              1|                              0.68|                              0.76|                              0.68|\n|                              2|                              1|                              0|                              1|                              1|                              2|                              0.68|                              0.77|                              0.69|\n|                              2|                              1|                              0|                              2|                              1|                              1|                              0.69|                              0.78|                              0.69|\n|                              2|                              1|                              0|                              2|                              1|                              2|                              0.69|                              0.79|                              0.69|\n|                              2|                              1|                              1|                              1|                              1|                              1|                              0.69|                              0.78|                              0.69|\n|                              2|                              1|                              1|                              1|                              1|                              2|                              0.69|                              0.80|                              0.69|\n|                              2|                              1|                              1|                              2|                              1|                              1|                              0.69|                              0.80|                              0.69|\n|                              2|                              1|                              1|                              2|                              1|                              2|                              0.70|                              0.82|                              0.70|\n|                              2|                              1|                              2|                              1|                              1|                              1|                              0.68|                              0.79|                              0.68|\n|                              2|                              1|                              2|                              1|                              1|                              2|                              0.68|                              0.80|                              0.68|\n|                              2|                              1|                              2|                              2|                              1|                              1|                              0.69|                              0.81|                              0.69|\n|                              2|                              1|                              2|                              2|                              1|                              2|                              0.69|                              0.82|                              0.69|\n:::\n:::\n\n\nFrom the results of the scores, it can be determined that the best model would be SARIMA(0,1,2,1,1,1).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit012 = Arima(price_ts, order=c(0,1,2), seasonal=c(1,1,1))\nsummary(fit012)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSeries: price_ts \nARIMA(0,1,2)(1,1,1)[12] \n\nCoefficients:\n         ma1     ma2     sar1    sma1\n      0.4220  0.1230  -0.0710  -1.000\ns.e.  0.0676  0.0713   0.0713   0.069\n\nsigma^2 = 0.09584:  log likelihood = -74.13\nAIC=158.26   AICc=158.52   BIC=175.49\n\nTraining set error measures:\n                      ME      RMSE       MAE        MPE     MAPE      MASE\nTraining set 0.004072782 0.2986528 0.1958307 0.02357084 3.312105 0.1574702\n                    ACF1\nTraining set -0.00670955\n```\n:::\n:::\n\n\n#### \n\n### Forecasting\n\nHaving the optimal SARIMA model, I can forecast the price of wheat for the next two years.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsarima.for(price_ts,24,0,1,2,1,1,1,12)\n```\n\n::: {.cell-output-display}\n![](arma_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n$pred\n          Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n2023 8.950891 9.058484 9.074394 9.041436 8.962985 8.766962 8.702220 8.872337\n2024 9.183499 9.346926 9.416358 9.404903 9.381704 9.103776 8.982589 9.130696\n          Sep      Oct      Nov      Dec\n2023 8.955317 9.052678 9.129206 9.145295\n2024 9.229079 9.345081 9.412628 9.414799\n\n$se\n           Jan       Feb       Mar       Apr       May       Jun       Jul\n2023 0.3145333 0.5467826 0.7315215 0.8782234 1.0037075 1.1151598 1.2164421\n2024 1.6990807 1.7642477 1.8268598 1.8873960 1.9460499 2.0029870 2.0583297\n           Aug       Sep       Oct       Nov       Dec\n2023 1.3098930 1.3969359 1.4788645 1.5564865 1.6304173\n2024 2.1121452 2.1643771 2.2153779 2.2652308 2.3140098\n```\n:::\n:::\n\n\nFrom the forecast, it seems that the future prices are stable, however the confidence band seems to be large.\n\n#### \n\n### Compare with Benchmark\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_ts = ts(usa_wheat$Price[1:196], start=c(2002, 8), frequency = 12)\ntest_ts = ts(usa_wheat$Price[197:245], start=c(2018, 12), frequency = 12)\n\nsarima_pred = sarima.for(train_ts,49,0,1,2,1,1,1,12,plot=FALSE)$pred\nsarima_res = sarima_pred-test_ts\nmean_pred = meanf(train_ts, h=49)\nmean_res = mean_pred$mean-test_ts\nnaive_pred = naive(train_ts, h=49)\nnaive_res = naive_pred$mean-test_ts\nsnaive_pred = snaive(train_ts, h=49)\nsnaive_res = snaive_pred$mean-test_ts\ndrift_pred = rwf(train_ts, h=49, drift=TRUE)\ndrift_res = drift_pred$mean-test_ts\n\n\nsarima_rmse = sqrt(mean(sarima_res**2))\nmean_rmse = sqrt(mean(mean_res**2))\nnaive_rmse = sqrt(mean(naive_res**2))\nsnaive_rmse = sqrt(mean(snaive_res**2))\ndrift_rmse = sqrt(mean(drift_res**2))\n\ncomparison = data.frame(\n  methods=c(\"SARIMA\", \"Mean\", \"Naive\", \"Snaive\", \"Drift\"),\n  RMSE=c(sarima_rmse, mean_rmse, naive_rmse, snaive_rmse, drift_rmse)\n)\n\nknitr::kable(comparison, format=\"pipe\", padding=30, digits=2)\n```\n\n::: {.cell-output-display}\n|methods                              |                              RMSE|\n|:------------------------------------|---------------------------------:|\n|SARIMA                               |                              2.11|\n|Mean                                 |                              2.22|\n|Naive                                |                              2.29|\n|Snaive                               |                              2.38|\n|Drift                                |                              2.10|\n:::\n:::\n\n\nFrom the numbers, the optimal SARIMA model has smaller RMSE than every benchmark methods except for drift method, meaning that it is performing really well on the wheat price time series.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(train_ts) +\n  autolayer(meanf(train_ts, h=49),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(train_ts, h=49),\n            series=\"Na誰ve\", PI=FALSE) +\n  autolayer(snaive(train_ts, h=49),\n            series=\"SNa誰ve\", PI=FALSE)+\n  autolayer(rwf(train_ts, h=49, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(sarima_pred, \n            series=\"SARIMA\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n```\n\n::: {.cell-output-display}\n![](arma_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\nIt seems that compared to the SARIMA(0,1,2,1,1,1) model the SNaive method's forecast is similar in shape, while the other three methods create simpler forecasts.\n\n#### \n\n### Seasonal Cross Validation\n\nFor this step, I will perform a seasonal cross validation on the model, first with 1 step ahead forecasts and then 12 (the seasonal period of the time series) step ahead forecast.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(140)\n\nfarima1 = function(x, h){forecast(Arima(x, order=c(0,1,2),seasonal = c(1,1,1)), h=h)}\ne1 = tsCV(price_ts, farima1, h=1)\nRMSE1 = sqrt(mean(e1^2, na.rm=TRUE))\nprint(RMSE1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.3747683\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfarima12 = function(x, h){forecast(Arima(x, order=c(0,1,2),seasonal = c(1,1,1)), h=h)}\ne12 = tsCV(price_ts, farima12, h=12)\nRMSE12 = sqrt(mean(e12^2, na.rm=TRUE))\nprint(RMSE12)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.467905\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(summary(fit012))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSeries: price_ts \nARIMA(0,1,2)(1,1,1)[12] \n\nCoefficients:\n         ma1     ma2     sar1    sma1\n      0.4220  0.1230  -0.0710  -1.000\ns.e.  0.0676  0.0713   0.0713   0.069\n\nsigma^2 = 0.09584:  log likelihood = -74.13\nAIC=158.26   AICc=158.52   BIC=175.49\n\nTraining set error measures:\n                      ME      RMSE       MAE        MPE     MAPE      MASE\nTraining set 0.004072782 0.2986528 0.1958307 0.02357084 3.312105 0.1574702\n                    ACF1\nTraining set -0.00670955\n```\n:::\n:::\n\n\nThe 12 steps ahead forecasting works by first using a part of the data as training set (1st,2nd,...,kth), and then using the (k+12)th observation as the validation. For the next iteration, the training set will then include the (k+1)th observation, where the validation set will be the (k+13)th observation. This procedure is repeated through the whole data set.\n\nFrom the results above, we can see that the forecasts using 1 step ahead have smaller RMSE than using 12 steps ahead, while the original model has the lowest RMSE.\n",
    "supporting": [
      "arma_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}