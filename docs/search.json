[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Haiyu Xiao",
    "section": "",
    "text": "Hello, I am Haiyu Xiao, a graduate student of data science and analytics program at Georgetown University. I went to University of Washington in Seattle during undergraduate, and I was studying applied mathematics. Seattle was a very beautiful city with a lot of place of interests, but I think Washington DC is just as cool. Since I got into my major during undergraduate, I have been learning about using math tools to solve problems in real life, and after joining the DSAN program here my professional interest had become even clearer: using different methods to find useful information from all sorts of data. This is also my basic definition of data science, as this modern world is creating large amount of data everyday, and these messy numbers are super beneficial as long as we use the right tools and procedures to make good use of them!\nThis time series project is designed to perform analyses on a topic by using multiple relevant models on time series data, so that it not only contributes to this specific topic, but it also represents what I learned from this class.\n\nContact Me\nhx111@georgetown.edu"
  },
  {
    "objectID": "arimax.html",
    "href": "arimax.html",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "",
    "text": "This tab will focus on fitting ARIMAX/SARIMAX model or VAR/VARMA model to find the relationships between the time series variables.\nTherefore, this part mainly answers the question how the measures of wheat affect the price of wheat (dollars per bushel) in USA. These measures include the yield (tonnes per hectare), and the amount of food use (bushels). These variables record the situation of wheat production and usage, therefore will have impacts on the price of wheat in USA.\n\n\n\n\nThe Variables\nFirst, read the data set, then make a plot of the time series predictors:\n\n\nCode\nwheat = read.csv(\"./HW4/USA_Wheat.csv\", sep=\",\")\nwheat = wheat[,c(1,3,4,5,6,7)]\nwheat$Yield = na.approx(wheat$Yield)\nwheat$Food_Use = as.numeric(gsub(\",\", \"\", wheat$Food_Use))\n# wheat$Import = as.numeric(gsub(\",\", \"\", wheat$Import))\n# wheat$Export = as.numeric(gsub(\",\", \"\", wheat$Export))\n\nstart_date = c(2002, 8)\nwheat_ts = ts(wheat, start=start_date, frequency=12)\n\nautoplot(wheat_ts[,c(3,2,4)], facets=TRUE)+\n  xlab(\"Time\")+ylab(\"\")+ggtitle(\"Time Series Plots of the Variables\")\n\n\n\n\n\nIn order to put the data on a better scale, I will do a log transformation:\n\n\nCode\nlg.wheat = wheat\nlg.wheat$Price = log(wheat$Price)\nlg.wheat$Yield = log(wheat$Yield)\nlg.wheat$Food_Use = log(wheat$Food_Use)\n# lg.wheat$Import = log(wheat$Import)\n# lg.wheat$Export = log(wheat$Export)\n\nwheat_ts = ts(lg.wheat, start=start_date, frequency=12)\n\nautoplot(wheat_ts[,c(3,2,4)], facets=TRUE)+\n  xlab(\"Time\")+ylab(\"\")+ggtitle(\"Time Series Plots of the Variables\")\n\n\n\n\n\nWe can see that the time series all have a increasing trend, while there is obvious seasonality for the amount of food use.\n\n\n\n\n\nFitting the Model Using `auto.arima`\nFirst, this function will be used to fit the ARIMAX model, and the exogenous (predictor) variables are Yield and Food_Use.\n\n\nCode\nxreg = cbind(yield = wheat_ts[,\"Yield\"],\n             food = wheat_ts[,\"Food_Use\"])\nfit_auto = auto.arima(wheat_ts[,\"Price\"], xreg=xreg)\nsummary(fit_auto)\n\n\nSeries: wheat_ts[, \"Price\"] \nRegression with ARIMA(0,1,2)(1,0,0)[12] errors \n\nCoefficients:\n         ma1     ma2     sar1    yield    food\n      0.4264  0.0807  -0.0103  -0.3092  0.1463\ns.e.  0.0665  0.0655   0.0684   0.1520  0.0551\n\nsigma^2 = 0.002313:  log likelihood = 396.68\nAIC=-781.36   AICc=-781   BIC=-760.37\n\nTraining set error measures:\n                      ME       RMSE        MAE       MPE     MAPE      MASE\nTraining set 0.002698144 0.04749694 0.03430143 0.1413272 2.063324 0.1604087\n                    ACF1\nTraining set -0.01739135\n\n\nCode\ncheckresiduals(fit_auto)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,2)(1,0,0)[12] errors\nQ* = 13.793, df = 21, p-value = 0.8783\n\nModel df: 3.   Total lags used: 24\n\n\nFrom the summary, the `auto.arima` function creates a SARIMAX model, a regression model with ARIMA(0,1,2)(1,0,0)[12] errors. From the Ljung-Box test, the residuals should be independent according to a p-value larger than 0.05, and this means the model fits the time series well. From the Q-Q plot we can also see that basically the residuals form a normal distribution.\n\n\n\n\n\nFitting the Model Manually\nHaving the linear regression model predicting wheat price using yield and food use, for the residuals I will fit an ARIMA/SARIMA model.\n\n\nCode\ntsdf = lg.wheat[,c(1:4)]\ntsdf$Yield = ts(tsdf$Yield, start=start_date, frequency=12)\ntsdf$Food_Use = ts(tsdf$Food_Use, start=start_date, frequency=12)\ntsdf$Price = ts(tsdf$Price, start=start_date, frequency=12)\n\nfit_reg = lm(Price~Yield+Food_Use, data=tsdf)\n# summary(fit_reg)\nfit_res = ts(residuals(fit_reg), start=start_date, frequency=12)\n\n\nLook at the residuals:\n\n\nCode\nacf(fit_res)\n\n\n\n\n\nCode\nPacf(fit_res)\n\n\n\n\n\nLooks like the residuals is heavily auto correlated and not stationary. So I will first try with a normal differencing:\n\n\nCode\nfit_res %>% diff() %>% ggtsdisplay()\n\n\n\n\n\nStill not good enough, therefore I will add a seasonal differencing:\n\n\nCode\nfit_res %>% diff() %>% diff(12) %>% ggtsdisplay()\n\n\n\n\n\n\n\n\n\n\nFinding the Model Parameters\nBased on the plots, I will choose a set of values for the parameters: d=D=1, p=[1,2,3], q=[1,2,3], P=[1,2], Q=[1,2].\n\n\nCode\nSARIMA.c = function(ps,qs,Ps,Qs,df){\n  scores = matrix(rep(NA, 9*36), nrow=36, ncol=9)\n  d=1\n  D=1\n  s=12\n  \n  i=1\n  for (p in ps){\n    for (q in qs){\n      for (P in Ps){\n        for (Q in Qs){\n          model = Arima(df, order=c(p-1,d,q-1), seasonal=c(P-1,D,Q-1))\n          scores[i,] = c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n          i=i+1\n        }\n      }\n    }\n  }\n  \n  scores = as.data.frame(scores)\n  colnames(scores) = c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  scores\n}\n\noutput = SARIMA.c(ps=c(1,2,3), qs=c(1,2,3), Ps=c(1,2), Qs=c(1,2), df=fit_res)\noutput\n\n\n   p d q P D Q       AIC       BIC      AICc\n1  0 1 0 0 1 0 -506.7858 -503.3391 -506.7684\n2  0 1 0 0 1 1 -643.4813 -636.5878 -643.4289\n3  0 1 0 1 1 0 -575.2285 -568.3351 -575.1761\n4  0 1 0 1 1 1 -643.7872 -633.4470 -643.6820\n5  0 1 1 0 1 0 -534.3747 -527.4812 -534.3223\n6  0 1 1 0 1 1 -662.7142 -652.3740 -662.6089\n7  0 1 1 1 1 0 -597.5658 -587.2256 -597.4606\n8  0 1 1 1 1 1 -662.1226 -648.3357 -661.9464\n9  0 1 2 0 1 0 -534.0560 -523.7158 -533.9508\n10 0 1 2 0 1 1 -663.3070 -649.5200 -663.1307\n11 0 1 2 1 1 0 -598.8966 -585.1096 -598.7204\n12 0 1 2 1 1 1 -662.9045 -645.6709 -662.6391\n13 1 1 0 0 1 0 -533.3746 -526.4811 -533.3222\n14 1 1 0 0 1 1 -663.8113 -653.4711 -663.7060\n15 1 1 0 1 1 0 -599.6166 -589.2764 -599.5113\n16 1 1 0 1 1 1 -663.3964 -649.6095 -663.2202\n17 1 1 1 0 1 0 -533.4232 -523.0830 -533.3179\n18 1 1 1 0 1 1 -662.1953 -648.4084 -662.0191\n19 1 1 1 1 1 0 -597.8183 -584.0314 -597.6421\n20 1 1 1 1 1 1 -661.6768 -644.4432 -661.4114\n21 1 1 2 0 1 0 -533.9757 -520.1888 -533.7995\n22 1 1 2 0 1 1 -663.2445 -646.0108 -662.9790\n23 1 1 2 1 1 0 -598.1932 -580.9596 -597.9278\n24 1 1 2 1 1 1 -662.9587 -642.2783 -662.5854\n25 2 1 0 0 1 0 -534.1136 -523.7734 -534.0083\n26 2 1 0 0 1 1 -662.3847 -648.5977 -662.2085\n27 2 1 0 1 1 0 -597.8694 -584.0824 -597.6932\n28 2 1 0 1 1 1 -661.8222 -644.5885 -661.5567\n29 2 1 1 0 1 0 -532.1390 -518.3521 -531.9628\n30 2 1 1 0 1 1 -660.2318 -642.9981 -659.9663\n31 2 1 1 1 1 0 -595.8844 -578.6507 -595.6189\n32 2 1 1 1 1 1 -659.9228 -639.2424 -659.5495\n33 2 1 2 0 1 0 -532.7373 -515.5036 -532.4718\n34 2 1 2 0 1 1 -662.2417 -641.5613 -661.8684\n35 2 1 2 1 1 0 -596.4577 -575.7773 -596.0843\n36 2 1 2 1 1 1 -662.0553 -637.9282 -661.5553\n\n\nIt is obvious that the best model here is ARIMA(1,1,0)(0,1,1)[12], while `auto.arima` suggested ARIMA(0,1,2)(1,0,0)[12].\nLet’s do some model diagnosis about this model.\n\n\nCode\nset.seed(140)\nmodel_output110 = capture.output(sarima(fit_res,1,1,0,0,1,1,12))\n\n\n\n\n\nCode\ncat(model_output110[23:52], model_output110[length(model_output110)], sep=\"\\n\")\n\n\nconverged\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1     sma1\n      0.3072  -1.0000\ns.e.  0.0634   0.1139\n\nsigma^2 estimated as 0.002791:  log likelihood = 334.91,  aic = -663.81\n\n$degrees_of_freedom\n[1] 230\n\n$ttable\n     Estimate     SE t.value p.value\nar1    0.3072 0.0634  4.8457       0\nsma1  -1.0000 0.1139 -8.7783       0\n\n$AIC\n[1] -2.861256\n\n$AICc\n[1] -2.86103\n\n$BIC\n\n\n\n\nCode\nmodel_output012 = capture.output(sarima(fit_res,0,1,2,1,0,0,12))\n\n\n\n\n\nCode\ncat(model_output012[20:50], model_output012[length(model_output110)], sep=\"\\n\")\n\n\niter   5 value -2.675943\niter   6 value -2.675944\niter   7 value -2.675945\niter   8 value -2.675946\niter   9 value -2.675947\niter  10 value -2.675947\niter  10 value -2.675947\nfinal  value -2.675947 \nconverged\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ma1     ma2    sar1  constant\n      0.2726  0.1086  0.5898    0.0038\ns.e.  0.0713  0.0729  0.0583    0.0137\n\nsigma^2 estimated as 0.004639:  log likelihood = 306.71,  aic = -603.42\n\n$degrees_of_freedom\n[1] 240\n\n$ttable\n         Estimate     SE t.value p.value\nma1        0.2726 0.0713  3.8225  0.0002\nma2        0.1086 0.0729  1.4896  0.1377\nsar1       0.5898 0.0583 10.1240  0.0000\n[1] -2.473033\n\n\n\n\n\n\n\nUse CV to Find the Best Model\nSince the two models above do not have much advantage over each other, in this step cross validation will be used to look for the optimal model, with RMSE plots.\n\n\nCode\nn = length(fit_res)\nk = 65\n\nrmse1 = rmse2 = matrix(NA,15,12)\nst = tsp(fit_res)[1]+(k-1)/12\n\nfor (i in 1:15){\n  xtrain = window(fit_res, end=st+i-1)\n  xtest = window(fit_res, start=st+i-1+1/12, end=st+i)\n  \n  fit1 = Arima(xtrain, order=c(1,1,0), seasonal=list(order=c(0,1,1),period=12), \n               include.drift=TRUE, method=\"ML\")\n  fcast1 = forecast(fit1, h=12)\n  \n  fit2 = Arima(xtrain, order=c(0,1,2), seasonal=list(order=c(1,0,0),period=12), \n               include.drift=TRUE, method=\"ML\")\n  fcast2 = forecast(fit2, h=12)\n  \n  rmse1[i,1:length(xtest)] = sqrt((fcast1$mean-xtest)^2)\n  rmse2[i,1:length(xtest)] = sqrt((fcast2$mean-xtest)^2)\n}\n\nplot(1:12, colMeans(rmse1, na.rm=TRUE), type=\"l\", col=2, xlab=\"Horizon\", ylab=\"RMSE\")\nlines(1:12, colMeans(rmse2, na.rm=TRUE), type=\"l\", col=3)\nlegend(\"topleft\", legend=c(\"my fit\", \"auto fit\"), col=2:3, lty=1)\n\n\n\n\n\nBased on the result of the cross validation, my optimal fit ARIMA(1,1,0)(0,1,1)[12] is better since it has lower RMSE.\n\n\nCode\nxreg = cbind(yield = tsdf[,\"Yield\"],\n             food = tsdf[,\"Food_Use\"])\nfit = Arima(tsdf[,\"Price\"], order=c(1,1,0), seasonal=c(0,1,1), xreg=xreg)\nsummary(fit)\n\n\nSeries: tsdf[, \"Price\"] \nRegression with ARIMA(1,1,0)(0,1,1)[12] errors \n\nCoefficients:\n         ar1     sma1    yield    food\n      0.3606  -1.0000  -0.2964  0.0762\ns.e.  0.0630   0.0782   0.1519  0.2169\n\nsigma^2 = 0.002217:  log likelihood = 363.64\nAIC=-717.29   AICc=-717.02   BIC=-700.05\n\nTraining set error measures:\n                        ME       RMSE        MAE         MPE     MAPE      MASE\nTraining set -7.128984e-05 0.04541963 0.03282718 -0.01445618 1.957385 0.1535144\n                   ACF1\nTraining set 0.04637381\n\n\nFrom the summary of the fit, it can be determined that the equation of the model is:\n\\[\ny_t = −0.2964x_{1,t}+0.0762x_{2,t}+u_t \\\\\nu_t = (1+0.3606B)(1−B^4)ε_t.\n\\]\n\n\n\n\n\nForecasting\nIn order to forecast the price of wheat, I need to have forecasts of yield and the food use first. Then the obtained forecasts will be used to find the prediction of the price.\n\n\nCode\nfit_yield = Arima(tsdf$Yield, order=c(1,1,0), seasonal=list(order=c(0,1,1),period=12))\nfyield = forecast(fit_yield, h=24)\nfit_food = Arima(tsdf$Food_Use, order=c(1,1,0), seasonal=list(order=c(0,1,1),period=12))\nffood = forecast(fit_food, h=24)\n\nfxreg = cbind(Yield=fyield$mean, Food=ffood$mean)\n\nfcast = forecast(fit, xreg=fxreg)\n\nautoplot(fcast)+xlab(\"Time\")+ylab(\"Price\")\n\n\n\n\n\nFrom the forecast plot, it seems that the prediction of the wheat price for the next two years follows the trend of the original time series, with a large confidence band. It seems that the result is not very nice."
  },
  {
    "objectID": "arma.html",
    "href": "arma.html",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "In this tab, I will focus on using these time series models, such as ARIMA (Auto-regressive integrated moving average) and SARIMA (Seasonal Auto-regressive Integrated Moving Average) to perform some forecasting on the crop production data"
  },
  {
    "objectID": "arma.html#usa-wheat-yield-arima-model",
    "href": "arma.html#usa-wheat-yield-arima-model",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "USA Wheat Yield (ARIMA Model)",
    "text": "USA Wheat Yield (ARIMA Model)\n\n\nCode\ncp = read.csv(\"./HW3/Monthly_Data.csv\")\n\nwheat_usa = cp[cp$Crop==\"Wheat\",]\ncolnames(wheat_usa)[4] = c(\"Yield\")\nwheat_usa$Yield = na.approx(wheat_usa$Yield)\n\ncorn_usa = cp[cp$Crop==\"Corn\",]\ncolnames(corn_usa)[4] = c(\"Yield\")\ncorn_usa$Yield = na.approx(corn_usa$Yield)\n\nstart_date = c(2002, 8)\nwheat_ts = ts(wheat_usa$Yield, start=start_date, frequency=12)\ncorn_ts = ts(corn_usa$Yield, start=start_date, frequency=12)\n\n\nFrom the exploratory data analysis part, there is some important information on the wheat yield data so it will be used in the ARIMA model experiment.\nAt the step where I plot the ACF and PACF plots, and where I performed the Augmented Dickey-Fuller Test, it is shown that the original time series is stationary. But to get rid of any of the seasonality, I will try doing one differencing on the time series.\n\n\nCode\ndiff_ts = diff(wheat_ts)\ntseries::adf.test(diff_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff_ts\nDickey-Fuller = -6.1309, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\nAgain, the Augmented Dickey-Fuller Test (ADF test) can confirm that the time series after differencing is clearly stationary with p-value=0.01.\n\n\n\n\nACF & PACF plots\nFirst, plot the ACF and PACF plots and decide the order of the AR(p) and MA(q) processes in the ARIMA(p,d,q) model.\n\n\nCode\nggAcf(diff_ts) + ggtitle(\"ACF of Monthly Wheat Production in USA\")\n\n\n\n\n\nCode\nggPacf(diff_ts) + ggtitle(\"PACF of Monthly Wheat Production in USA\")\n\n\n\n\n\nThen, from the ACF and PACF graphs, I will choose a set of values for the parameters: p=(0:1) and q=(0:4). Since I did one differencing to the time series, d=1 in this case.\n\n\n\n\n\nFit ARIMA with choices of parameters\nThen, I will fit my choice of ARIMA(p,d,q) to the data, and select the model with lowest AIC, BIC.\n\n\nCode\nscores = matrix(rep(0, 5*10), nrow=10, ncol=5)\n\nd=1\nfor (p in 0:1){\n  for (q in 0:4){\n    model = Arima(wheat_ts, order=c(p,d,q), include.drift=TRUE)\n    scores[p*5+q+1,] = c(p,q,model$aic, model$bic, model$aicc)\n  }\n}\n\nscores = as.data.frame(scores)\ncolnames(scores) = c(\"p\",\"q\",\"AIC\",\"BIC\",\"AICC\")\nknitr::kable(scores, format=\"pipe\", padding=30, digits=2)\n\n\n\n\n\n\n\n\n\n\n\n\np\nq\nAIC\nBIC\nAICC\n\n\n\n\n0\n0\n-679.11\n-672.11\n-679.06\n\n\n0\n1\n-686.67\n-676.18\n-686.57\n\n\n0\n2\n-686.04\n-672.05\n-685.87\n\n\n0\n3\n-685.74\n-668.25\n-685.48\n\n\n0\n4\n-683.87\n-662.89\n-683.51\n\n\n1\n0\n-687.41\n-676.92\n-687.31\n\n\n1\n1\n-685.41\n-671.42\n-685.25\n\n\n1\n2\n-685.48\n-668.00\n-685.23\n\n\n1\n3\n-683.87\n-662.89\n-683.52\n\n\n1\n4\n-681.87\n-657.39\n-681.39\n\n\n\n\n\nFrom the results of the AIC and BIC scores, I can determine that the model with the lowest AIC, BIC has parameters p=1, q=0. Therefore, the model will be ARIMA(1,1,0).\n\n\nCode\nfit10 = Arima(wheat_ts, order=c(1,1,0), include.drift=TRUE)\nsummary(fit10)\n\n\nSeries: wheat_ts \nARIMA(1,1,0) with drift \n\nCoefficients:\n         ar1   drift\n      0.2029  0.0031\ns.e.  0.0625  0.0047\n\nsigma^2 = 0.003442:  log likelihood = 346.71\nAIC=-687.41   AICc=-687.31   BIC=-676.92\n\nTraining set error measures:\n                       ME       RMSE       MAE          MPE     MAPE      MASE\nTraining set 8.570452e-06 0.05830938 0.0328281 -0.006918382 1.102147 0.1870143\n                     ACF1\nTraining set 0.0004080243\n\n\nThen, the equation of this model is:\n\\[\n(X_t-0.203X_{t-1}=w_t)\n\\]\n\n\n\n\n\nModel Diagnostic\nThis part contains some technique overview and diagnostic about this model. I will use some plots to describe about the residuals.\n\n\nCode\nmodel_output <- capture.output(sarima(wheat_ts,1,1,0))\n\n\n\n\n\n\n\nCode\ncheckresiduals(fit10)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(1,1,0) with drift\nQ* = 27.624, df = 23, p-value = 0.2303\n\nModel df: 1.   Total lags used: 24\n\n\nHere is the result of checking residual of the ARIMA(1,1,0) model. From the Ljung-Box test, the residuals should be independent according to a p-value larger than 0.05, and this means the model fits the time series well. From the Q-Q plot we can also see basically the residuals form a normal distribution.\n\n\n\n\n\nCompare with auto.arima()\nTo test if my model agrees with the best model chosen by the auto process, I will try to use a `auto.arima()` function for testing.\n\n\nCode\nauto.arima(wheat_ts, seasonal=FALSE, ic = \"aic\")\n\n\nSeries: wheat_ts \nARIMA(0,1,1) \n\nCoefficients:\n         ma1\n      0.1876\ns.e.  0.0577\n\nsigma^2 = 0.003445:  log likelihood = 346.1\nAIC=-688.2   AICc=-688.15   BIC=-681.2\n\n\nApparently the result is different from my chosen model, and the AIC and BIC of the auto result is also larger. This may be because the `auto.arima` function does not search through enough possible parameters to find the model with the best statistics.\n\n\n\n\n\nForecasting\nNow having my optimal model I can forecast using it with a confidence band. I will then plot the forecasts.\n\n\nCode\npred = forecast(fit10, 24)\npred_ts = ts(pred$mean, start=c(2023,1), frequency=12)\nts.plot(wheat_ts, pred_ts, gpars = list(col = c(\"black\", \"red\")))\n\n\n\n\n\nThe forecasting result of 2 years show that there will be a steady increase in the yield of wheat, which follows the general trend of the data, however the actual yield might not look as simple.\n\n\n\n\n\nCompare with Benchmark\nHere, I will compare my optimal ARIMA model with other benchmark methods such as mean, naive, snaive and drift, using MAE and RMSE.\nI plan to do this with training and testing sets, where I can train the different models and methods on the training set, and the RMSE of the models on the testing set can be reported and compared.\n\n\nCode\ntrain_ts = ts(wheat_usa$Yield[1:196], start=c(2002, 8), frequency = 12)\ntest_ts = ts(wheat_usa$Yield[197:245], start=c(2018, 12), frequency = 12)\n\narima_train = Arima(train_ts, order=c(1,1,0), include.drift = TRUE)\narima_pred = forecast(arima_train,49)\narima_res = arima_pred$mean-test_ts\n\nmean_pred = meanf(train_ts, h=49)\nmean_res = mean_pred$mean-test_ts\nnaive_pred = naive(train_ts, h=49)\nnaive_res = naive_pred$mean-test_ts\nsnaive_pred = snaive(train_ts, h=49)\nsnaive_res = snaive_pred$mean-test_ts\ndrift_pred = rwf(train_ts, h=49, drift=TRUE)\ndrift_res = drift_pred$mean-test_ts\n\n\n\n\nCode\narima_rmse = sqrt(mean(arima_res**2))\nmean_rmse = sqrt(mean(mean_res**2))\nnaive_rmse = sqrt(mean(naive_res**2))\nsnaive_rmse = sqrt(mean(snaive_res**2))\ndrift_rmse = sqrt(mean(drift_res**2))\n\ncomparison = data.frame(\n  methods=c(\"ARIMA\", \"Mean\", \"Naive\", \"Snaive\", \"Drift\"),\n  RMSE=c(arima_rmse, mean_rmse, naive_rmse, snaive_rmse, drift_rmse)\n)\n\nknitr::kable(comparison, format=\"pipe\", padding=30, digits=2)\n\n\n\n\n\n\n\n\n\nmethods\nRMSE\n\n\n\n\nARIMA\n0.20\n\n\nMean\n0.34\n\n\nNaive\n0.15\n\n\nSnaive\n0.18\n\n\nDrift\n0.20\n\n\n\n\n\nBased on the result of the comparison, the RMSE of the optimal ARIMA model is much less than the mean method, the same as the Drift method, but a little larger than Naive and Snaive method. The numbers here show that the ARIMA model has a decent performance on the wheat production time series.\nThen, I will compare the forcasts results using a plot.\n\n\nCode\nautoplot(train_ts) +\n  autolayer(meanf(train_ts, h=49),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(train_ts, h=49),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(train_ts, h=49),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(train_ts, h=49, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(arima_train, 49), \n            series=\"ARIMA\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\nIt seems that the ARIMA(1,1,0) model gives a forecast that is simple and basically follows the previous trend, and is basically the same as the forecast of the drift method. Except for mean method which is far off, the other methods give relatively reliable forecasts."
  },
  {
    "objectID": "arma.html#usa-wheat-price-sarima-model",
    "href": "arma.html#usa-wheat-price-sarima-model",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "USA Wheat Price (SARIMA Model)",
    "text": "USA Wheat Price (SARIMA Model)\nAt this step I will move on to another focus of this project, the price of the wheat in the United States. First, look at the plot and the decompose of the wheat price time series:\n\n\nCode\nusa_wheat = read.csv(\"./HW4/USA_Wheat.csv\")\nusa_wheat$Price = na.approx(usa_wheat$Price)\n\nstart_date = c(2002, 8)\nprice_ts = ts(usa_wheat$Price, start=start_date, frequency=12)\n\nautoplot(price_ts)+xlab(\"Date\")+ylab(\"Price\")\n\n\n\n\n\nCode\nautoplot(decompose(price_ts, \"additive\"))\n\n\n\n\n\nIt looks like the trend of the wheat price is not clear through out the years, and the seasonality is obvious.\n\n\n\n\nCheck for stationarity\nTo check the seasonal component and the stationarity, I will use a ACF graph and a test:\n\n\nCode\nggAcf(price_ts,200)\n\n\n\n\n\nCode\ntseries::adf.test(price_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  price_ts\nDickey-Fuller = -2.8178, Lag order = 6, p-value = 0.2317\nalternative hypothesis: stationary\n\n\nIt looks like the time series may have seasonal affect, and is definitely not stationary. Therefore, I will do a seasonal difference in addition to a general differencing to the time series.\n\n\nCode\nprice_ts %>% diff(lag=12) %>% diff() %>% ggtsdisplay()\n\n\n\n\n\n\n\n\n\n\nLook for the Best Model\nBased on the ACF and the PACF plots, it can be inferred that d=1, D=1, p=[0,1,2], q=[0,1,2], P=[1,2], Q=[1,2]. Then, I will fit my choice of SARIMA(p,d,q,P,D,Q) to the data, and select the model with lowest AIC, BIC.\n\n\nCode\nscores = matrix(rep(0, 9*36), nrow=36, ncol=9)\n\nd=1\nD=1\ni=1\nfor (p in 0:2){\n  for (q in 0:2){\n    for (P in 1:2){\n      for (Q in 1:2) {\n        model = sarima(price_ts,p,d,q,P,D,Q,S=12,details=FALSE)\n        scores[i,] = c(p,d,q,P,D,Q,model$AIC,model$BIC,model$AICc)\n        i = i+1\n      }\n\n    }\n  }\n}\n\nscores = as.data.frame(scores)\ncolnames(scores) = c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICC\")\nknitr::kable(scores, format=\"pipe\", padding=30, digits=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICC\n\n\n\n\n0\n1\n0\n1\n1\n1\n0.82\n0.87\n0.82\n\n\n0\n1\n0\n1\n1\n2\n0.83\n0.89\n0.83\n\n\n0\n1\n0\n2\n1\n1\n0.83\n0.89\n0.83\n\n\n0\n1\n0\n2\n1\n2\n0.83\n0.90\n0.83\n\n\n0\n1\n1\n1\n1\n1\n0.69\n0.75\n0.69\n\n\n0\n1\n1\n1\n1\n2\n0.69\n0.76\n0.69\n\n\n0\n1\n1\n2\n1\n1\n0.69\n0.76\n0.69\n\n\n0\n1\n1\n2\n1\n2\n0.69\n0.78\n0.70\n\n\n0\n1\n2\n1\n1\n1\n0.68\n0.76\n0.68\n\n\n0\n1\n2\n1\n1\n2\n0.69\n0.77\n0.69\n\n\n0\n1\n2\n2\n1\n1\n0.69\n0.78\n0.69\n\n\n0\n1\n2\n2\n1\n2\n0.69\n0.79\n0.69\n\n\n1\n1\n0\n1\n1\n1\n0.69\n0.75\n0.69\n\n\n1\n1\n0\n1\n1\n2\n0.70\n0.77\n0.70\n\n\n1\n1\n0\n2\n1\n1\n0.69\n0.77\n0.70\n\n\n1\n1\n0\n2\n1\n2\n0.70\n0.79\n0.70\n\n\n1\n1\n1\n1\n1\n1\n0.69\n0.76\n0.69\n\n\n1\n1\n1\n1\n1\n2\n0.69\n0.78\n0.70\n\n\n1\n1\n1\n2\n1\n1\n0.69\n0.78\n0.69\n\n\n1\n1\n1\n2\n1\n2\n0.70\n0.80\n0.70\n\n\n1\n1\n2\n1\n1\n1\n0.69\n0.78\n0.69\n\n\n1\n1\n2\n1\n1\n2\n0.69\n0.79\n0.69\n\n\n1\n1\n2\n2\n1\n1\n0.69\n0.80\n0.69\n\n\n1\n1\n2\n2\n1\n2\n0.70\n0.81\n0.70\n\n\n2\n1\n0\n1\n1\n1\n0.68\n0.76\n0.68\n\n\n2\n1\n0\n1\n1\n2\n0.68\n0.77\n0.69\n\n\n2\n1\n0\n2\n1\n1\n0.69\n0.78\n0.69\n\n\n2\n1\n0\n2\n1\n2\n0.69\n0.79\n0.69\n\n\n2\n1\n1\n1\n1\n1\n0.69\n0.78\n0.69\n\n\n2\n1\n1\n1\n1\n2\n0.69\n0.80\n0.69\n\n\n2\n1\n1\n2\n1\n1\n0.69\n0.80\n0.69\n\n\n2\n1\n1\n2\n1\n2\n0.70\n0.82\n0.70\n\n\n2\n1\n2\n1\n1\n1\n0.68\n0.79\n0.68\n\n\n2\n1\n2\n1\n1\n2\n0.68\n0.80\n0.68\n\n\n2\n1\n2\n2\n1\n1\n0.69\n0.81\n0.69\n\n\n2\n1\n2\n2\n1\n2\n0.69\n0.82\n0.69\n\n\n\n\n\nFrom the results of the scores, it can be determined that the best model would be SARIMA(0,1,2,1,1,1).\n\n\nCode\nfit012 = Arima(price_ts, order=c(0,1,2), seasonal=c(1,1,1))\nsummary(fit012)\n\n\nSeries: price_ts \nARIMA(0,1,2)(1,1,1)[12] \n\nCoefficients:\n         ma1     ma2     sar1    sma1\n      0.4220  0.1230  -0.0710  -1.000\ns.e.  0.0676  0.0713   0.0713   0.069\n\nsigma^2 = 0.09584:  log likelihood = -74.13\nAIC=158.26   AICc=158.52   BIC=175.49\n\nTraining set error measures:\n                      ME      RMSE       MAE        MPE     MAPE      MASE\nTraining set 0.004072782 0.2986528 0.1958307 0.02357084 3.312105 0.1574702\n                    ACF1\nTraining set -0.00670955\n\n\n\n\n\n\n\nForecasting\nHaving the optimal SARIMA model, I can forecast the price of wheat for the next two years.\n\n\nCode\nsarima.for(price_ts,24,0,1,2,1,1,1,12)\n\n\n\n\n\n$pred\n          Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n2023 8.950891 9.058484 9.074394 9.041436 8.962985 8.766962 8.702220 8.872337\n2024 9.183499 9.346926 9.416358 9.404903 9.381704 9.103776 8.982589 9.130696\n          Sep      Oct      Nov      Dec\n2023 8.955317 9.052678 9.129206 9.145295\n2024 9.229079 9.345081 9.412628 9.414799\n\n$se\n           Jan       Feb       Mar       Apr       May       Jun       Jul\n2023 0.3145333 0.5467826 0.7315215 0.8782234 1.0037075 1.1151598 1.2164421\n2024 1.6990807 1.7642477 1.8268598 1.8873960 1.9460499 2.0029870 2.0583297\n           Aug       Sep       Oct       Nov       Dec\n2023 1.3098930 1.3969359 1.4788645 1.5564865 1.6304173\n2024 2.1121452 2.1643771 2.2153779 2.2652308 2.3140098\n\n\nFrom the forecast, it seems that the future prices are stable, however the confidence band seems to be large.\n\n\n\n\n\nCompare with Benchmark\n\n\nCode\ntrain_ts = ts(usa_wheat$Price[1:196], start=c(2002, 8), frequency = 12)\ntest_ts = ts(usa_wheat$Price[197:245], start=c(2018, 12), frequency = 12)\n\nsarima_pred = sarima.for(train_ts,49,0,1,2,1,1,1,12,plot=FALSE)$pred\nsarima_res = sarima_pred-test_ts\nmean_pred = meanf(train_ts, h=49)\nmean_res = mean_pred$mean-test_ts\nnaive_pred = naive(train_ts, h=49)\nnaive_res = naive_pred$mean-test_ts\nsnaive_pred = snaive(train_ts, h=49)\nsnaive_res = snaive_pred$mean-test_ts\ndrift_pred = rwf(train_ts, h=49, drift=TRUE)\ndrift_res = drift_pred$mean-test_ts\n\n\nsarima_rmse = sqrt(mean(sarima_res**2))\nmean_rmse = sqrt(mean(mean_res**2))\nnaive_rmse = sqrt(mean(naive_res**2))\nsnaive_rmse = sqrt(mean(snaive_res**2))\ndrift_rmse = sqrt(mean(drift_res**2))\n\ncomparison = data.frame(\n  methods=c(\"SARIMA\", \"Mean\", \"Naive\", \"Snaive\", \"Drift\"),\n  RMSE=c(sarima_rmse, mean_rmse, naive_rmse, snaive_rmse, drift_rmse)\n)\n\nknitr::kable(comparison, format=\"pipe\", padding=30, digits=2)\n\n\n\n\n\n\n\n\n\nmethods\nRMSE\n\n\n\n\nSARIMA\n2.11\n\n\nMean\n2.22\n\n\nNaive\n2.29\n\n\nSnaive\n2.38\n\n\nDrift\n2.10\n\n\n\n\n\nFrom the numbers, the optimal SARIMA model has smaller RMSE than every benchmark methods except for drift method, meaning that it is performing really well on the wheat price time series.\n\n\nCode\nautoplot(train_ts) +\n  autolayer(meanf(train_ts, h=49),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(train_ts, h=49),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(train_ts, h=49),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(train_ts, h=49, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(sarima_pred, \n            series=\"SARIMA\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\nIt seems that compared to the SARIMA(0,1,2,1,1,1) model the SNaive method’s forecast is similar in shape, while the other three methods create simpler forecasts.\n\n\n\n\n\nSeasonal Cross Validation\nFor this step, I will perform a seasonal cross validation on the model, first with 1 step ahead forecasts and then 12 (the seasonal period of the time series) step ahead forecast.\n\n\nCode\nset.seed(140)\n\nfarima1 = function(x, h){forecast(Arima(x, order=c(0,1,2),seasonal = c(1,1,1)), h=h)}\ne1 = tsCV(price_ts, farima1, h=1)\nRMSE1 = sqrt(mean(e1^2, na.rm=TRUE))\nprint(RMSE1)\n\n\n[1] 0.3747683\n\n\n\n\nCode\nfarima12 = function(x, h){forecast(Arima(x, order=c(0,1,2),seasonal = c(1,1,1)), h=h)}\ne12 = tsCV(price_ts, farima12, h=12)\nRMSE12 = sqrt(mean(e12^2, na.rm=TRUE))\nprint(RMSE12)\n\n\n[1] 1.467905\n\n\n\n\nCode\nprint(summary(fit012))\n\n\nSeries: price_ts \nARIMA(0,1,2)(1,1,1)[12] \n\nCoefficients:\n         ma1     ma2     sar1    sma1\n      0.4220  0.1230  -0.0710  -1.000\ns.e.  0.0676  0.0713   0.0713   0.069\n\nsigma^2 = 0.09584:  log likelihood = -74.13\nAIC=158.26   AICc=158.52   BIC=175.49\n\nTraining set error measures:\n                      ME      RMSE       MAE        MPE     MAPE      MASE\nTraining set 0.004072782 0.2986528 0.1958307 0.02357084 3.312105 0.1574702\n                    ACF1\nTraining set -0.00670955\n\n\nThe 12 steps ahead forecasting works by first using a part of the data as training set (1st,2nd,…,kth), and then using the (k+12)th observation as the validation. For the next iteration, the training set will then include the (k+1)th observation, where the validation set will be the (k+13)th observation. This procedure is repeated through the whole data set.\nFrom the results above, we can see that the forecasts using 1 step ahead have smaller RMSE than using 12 steps ahead, while the original model has the lowest RMSE."
  },
  {
    "objectID": "conclusions.html",
    "href": "conclusions.html",
    "title": "Conclusions",
    "section": "",
    "text": "To conclude this time series project, let’s first see a diagram of the workflow of the data story:"
  },
  {
    "objectID": "conclusions.html#data-story",
    "href": "conclusions.html#data-story",
    "title": "Conclusions",
    "section": "Data Story",
    "text": "Data Story\nStarted from gathering data, then performing visualizations and exploratory analysis on the data, as well as implementing time series models, we can now provide answers to the data science questions, and tell a complete data story about crops yield and price prediction. First, we look at the the yield of different kinds of crops in some big countries to have a sense of the situation, and we move on to focus on the yield of wheat in the US and its prediction. Then, since the price of the wheat in the US may be affected by many factors, the yield and amount of food use of wheat and their impacts will be considered when predicting the price of wheat in the US."
  },
  {
    "objectID": "conclusions.html#conclusions",
    "href": "conclusions.html#conclusions",
    "title": "Conclusions",
    "section": "Conclusions",
    "text": "Conclusions\nThe trends are generally increasing for the yield of different crops in the big countries in this century, and the United States is one of the countries with the most increase. The US also has higher yield than other countries for almost all kinds of crops, including wheat.\nThen, let’s look at the situations of the yield of wheat in the US. The general trend is going up for most of the time, increasing from 2.4 to 3.3 tonnes/hectare, and there is a big increase during 2016 to 2017. as there is also clear seasonality. Other basic plotting shows that the wheat data in USA is non random, non-stationary.\nThe forecasting of the yield of wheat in the US was performed by both ARIMA models and deep learning models. ARIMA(1,1,0) was chosen by selecting the right parameters, and the comparison with benchmark methods shows that it has decent performance. Then the ARIMA model provides the forecast of the yield to be steadily increasing for the next two years. The deep learning models, RNN, GRU and LSTM are also implemented to forecast the yield. While all the ANN models are providing more detailed and accurate predictions than the ARIMA model, RNN and GRU are the better performing models on this task.\nThe forecasting of the price of wheat in the US was performed by SARIMA and SARIMAX models. SARIMA(0,1,2,1,1,1,12) model was chosen by a similar approach, and after comparing with benchmark methods and doing seasonal cross validation, this model proves to be the best performing SARIMA model, and it forecasts the price of wheat to be at the same level for the next two years. The SARIMAX model was implemented by first binding the yield and food use of wheat, and then fitting the model using auto and manual approaches with cross validation. Finally, the forecast of the price was obtained using the individual forecasts of the yield and food use, and the fact that the forecast here is similar to the forecast of the SARIMA model proves that the yield and food use of wheat in the US are the important factors that can be used to predict the price."
  },
  {
    "objectID": "data_sources.html",
    "href": "data_sources.html",
    "title": "Data Sources",
    "section": "",
    "text": "A number of data were gathered using different sources for this project. I searched websites like data.world and ICPSR, and gathered data sets from OECD, USDA etc. for the crops production and price data.\n\n\n\n\nOECD - Crop Production Data\nOrganisation for Economic Co-operation and Development is an intergovernmental organization founded to observe and stimulate economic progress. They own lots of data sets recording all sorts of indices related to economics, ranging from adults education level to greenhouse gas emissions. I found this nice time series data set that documents the yields of different crops from many countries, which is suitable for predicting yields of each type of crops.\nThe data can be downloaded here. Yields values are measured in tonnes per hectare.\nThe source of the data can be viewed here.\n\n\n\n\n\nUSDA - Wheat Data\nU.S. DEPARTMENT OF AGRICULTURE is providing services and resources for economic research on agriculture, and the wheat data on their website is very useful for the project. There are multiple data sets used, such as the price of wheat received by farmers and the import and export quantities. These variables are very suitable for time series analysis.\nThe data can be downloaded here.\nThe source of the data can be viewed here.\n\n\n\n\n\nYahoo Finance - Wheat Stocks\nWheat farming takes up more land than almost any other commodity on a global scale. Because of the volume of grain produced each year, numerous businesses make a living by growing, transporting, or selling it. Stocks related to wheat production and price can be useful for studying financial time series of wheat. Source"
  },
  {
    "objectID": "data_visualization.html#crop-production-data",
    "href": "data_visualization.html#crop-production-data",
    "title": "Data Visualization",
    "section": "Crop Production Data",
    "text": "Crop Production Data\nFor the crop production data, I will visualize this data by comparing the yields of different crops from one country over time, or I will also show how are the yields differ for some countries/regions. In this part I mainly use ggplot and plot-ly.\n\nUSA Crop Yields\nI choose United States from example to show how the yields of the crops differ through out the years.\nFirst, let’s look at the original data of the yields (in tonnes per hectare):\n\n\nCode\ncp = read.csv(\"./HW2/crop_production.csv\")\ncp_usa = cp[cp$LOCATION==\"USA\" & cp$MEASURE==\"TONNE_HA\", -c(2,4,5,8)]\nrow.names(cp_usa) <- NULL\nh1 = head(cp_usa[cp_usa$SUBJECT==\"RICE\",])\nh2 = head(cp_usa[cp_usa$SUBJECT==\"WHEAT\",])\nknitr::kable(list(h1,h2), format=\"pipe\", padding=30, digits=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLOCATION\nSUBJECT\nTIME\nValue\n\n\n\n\nUSA\nRICE\n1990\n4.31\n\n\nUSA\nRICE\n1991\n4.47\n\n\nUSA\nRICE\n1992\n4.47\n\n\nUSA\nRICE\n1993\n4.30\n\n\nUSA\nRICE\n1994\n4.65\n\n\nUSA\nRICE\n1995\n4.38\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLOCATION\nSUBJECT\nTIME\nValue\n\n\n\n\n37\nUSA\nWHEAT\n1990\n2.66\n\n\n38\nUSA\nWHEAT\n1991\n2.30\n\n\n39\nUSA\nWHEAT\n1992\n2.64\n\n\n40\nUSA\nWHEAT\n1993\n2.57\n\n\n41\nUSA\nWHEAT\n1994\n2.53\n\n\n42\nUSA\nWHEAT\n1995\n2.41\n\n\n\n\n\n\n\n\n\n\nObviously, the yields of rice are higher than wheat in the US. To discover the behaviors and find out whether there are any specific situations, I will plot the annual yields of the four types of crops in USA:\n\n\nCode\ng1 <- ggplot(data=NULL, aes(x=cp_usa$TIME[cp_usa$SUBJECT==\"RICE\"])) +\n  geom_line(aes(y=cp_usa$Value[cp_usa$SUBJECT==\"RICE\"], colour=\"RICE\"))+\n  geom_line(aes(y=cp_usa$Value[cp_usa$SUBJECT==\"WHEAT\"], colour=\"WHEAT\"))+\n  geom_line(aes(y=cp_usa$Value[cp_usa$SUBJECT==\"SOYBEAN\"], colour=\"SOYBEAN\"))+\n  geom_line(aes(y=cp_usa$Value[cp_usa$SUBJECT==\"MAIZE\"], colour=\"MAIZE\"))+\n  ggtitle(\"Annual Yields of the Four Crops in USA\")+\n  theme(plot.title = element_text(size = 14))+\n  labs(x = \"Year\", y = \"Production (Tons/Hectare)\")+\n  guides(colour=guide_legend(title=\"Crops\")) \n\nggplotly(g1) %>%\n  layout(hovermode = \"x\")\n\n\n\n\n\n\nFrom 1990 to today, in the US the production of maize (corn) is always the highest, with rice being the second, and wheat and soybean being at the same level. All four crops’ yields increased with different degrees, with maize’s yield per hectare the most.\nThe yield of maize had a drop during 2011-2012, however I can’t find a specific reason for this while the best I can think of is the effect of economic crisis on agriculture.\n\n\nCompare Among Countries\nI choose some of the most crop producing countries in the world, since I want to compare that for the different crops how are their yields differ over time.\nFirst, from the big crop production data frame I choose only the yields of the crops in the selected countries:\n\n\nCode\ncp_countries = cp[cp$MEASURE==\"TONNE_HA\" & (cp$LOCATION %in% c(\"USA\",\"CHN\",\"IND\",\"BRA\",\"RUS\",\"MEX\",\"JPN\",\"\")), -c(2,4,5,8)]\nrow.names(cp_countries) <- NULL\nhead(cp_countries)\n\n\n  LOCATION SUBJECT TIME    Value\n1      JPN    RICE 1990 4.586352\n2      JPN    RICE 1991 4.246571\n3      JPN    RICE 1992 4.548499\n4      JPN    RICE 1993 3.318188\n5      JPN    RICE 1994 4.907227\n6      JPN    RICE 1995 4.597586\n\n\nThen, using plot-ly I create an animated bar plot of the yields, where the bars represent different crops.\n\n\nCode\nfig2 = cp_countries %>%\n  plot_ly(x=~LOCATION, y=~Value, frame=~TIME, color=~SUBJECT, type=\"bar\")\n\nfig2 %>% layout(title = \"Yield of the 4 Crops in Each Country Since 1990\", xaxis = list(title='Year', showgrid=F), yaxis = list(title='Yield', showgrid=F))\n\n\n\n\n\n\nSince yield data from USA is way too conspicuous here, I want to focus on the changes over time of the other countries.\n\n\nCode\ncp_countries = cp[cp$MEASURE==\"TONNE_HA\" & (cp$LOCATION %in% c(\"CHN\",\"IND\",\"BRA\",\"RUS\",\"MEX\",\"JPN\",\"\")), -c(2,4,5,8)]\nrow.names(cp_countries) <- NULL\n\nfig3 = cp_countries %>%\n  plot_ly(x=~LOCATION, y=~Value, frame=~TIME, color=~SUBJECT, type=\"bar\")\n\nfig3 %>% layout(title = \"Yield of the 4 Crops in Each Country Since 1990\", xaxis = list(title='Year', showgrid=F), yaxis = list(title='Yield', showgrid=F))\n\n\n\n\n\n\nSome key points that we can discover from the plot are:\n\nThe total yields of USA are way higher than any other countries, especially maize.\nAlmost every country had soybean as their least produced crop, but how much the other crops are produced each year basically match how the diet is like in that country.\nFrom 1990 to the 2020s, the countries with the most increase in yields are: Brazil, China and USA; the countries with less are India, Japan, Mexico, with Russia had moderate grow. Particularly, the crops in Japan and Mexico basically increased less than 1 ton/hectare, while the other countries had more than that. This may be because that Japan and Mexico had lower economic growth since 1990 than others."
  },
  {
    "objectID": "deep_learning.html",
    "href": "deep_learning.html",
    "title": "Deep Learning for TS",
    "section": "",
    "text": "Deep Learning for TS"
  },
  {
    "objectID": "dv.html#data-visualization-with-stock-data",
    "href": "dv.html#data-visualization-with-stock-data",
    "title": "Data Visuals in TS",
    "section": "Data Visualization with Stock Data",
    "text": "Data Visualization with Stock Data\nThis section visualizes the stock data of Google and Microsoft from 01/01/2020 to 12/01/2020.\n\nData set:\n\n\n              GOOG     MSFT       date\n2020-01-02 68.3685 155.7618 2020-01-02\n2020-01-03 68.0330 153.8223 2020-01-03\n2020-01-06 69.7105 154.2199 2020-01-06\n2020-01-07 69.6670 152.8138 2020-01-07\n2020-01-08 70.2160 155.2478 2020-01-08\n2020-01-09 70.9915 157.1874 2020-01-09\n\n\n\n\nPlot:"
  },
  {
    "objectID": "dv.html#plot-climate-data",
    "href": "dv.html#plot-climate-data",
    "title": "Data Visuals in TS",
    "section": "Plot Climate Data",
    "text": "Plot Climate Data\nFor this part, I used the max and min temperature of each day at National Arboretum DC, MD US as the climate data. Time range was 01/01/2021 to 09/01/2021.\n\nData set:\n\n\n                            NAME TMAX TMIN       date\n169 NATIONAL ARBORETUM DC, MD US   54   36 2021-01-01\n170 NATIONAL ARBORETUM DC, MD US   43   35 2021-01-02\n171 NATIONAL ARBORETUM DC, MD US   57   38 2021-01-03\n172 NATIONAL ARBORETUM DC, MD US   42   32 2021-01-04\n173 NATIONAL ARBORETUM DC, MD US   46   32 2021-01-05\n174 NATIONAL ARBORETUM DC, MD US   42   39 2021-01-06\n\n\n\n\nPlot:"
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "The exploratory data analysis part will focus on finding the time series components of the yields data."
  },
  {
    "objectID": "eda.html#crop-production-data",
    "href": "eda.html#crop-production-data",
    "title": "Exploratory Data Analysis",
    "section": "Crop Production Data",
    "text": "Crop Production Data\nIn this example, I will use the monthly yield of wheat to show the time series plot, and the time series components, such as the trend, seasonality and stationarity, etc.\nFirst, read in the data, and replace the NA values. I choose to fix them with linear interpolation.\n\n\nCode\ncp = read.csv(\"./HW2/Monthly_Data.csv\")\n\nwheat_usa = cp[cp$Crop==\"Wheat\",]\ncolnames(wheat_usa)[4] = c(\"Yield\")\nwheat_usa$Yield = na.approx(wheat_usa$Yield)\n\n\nThen, a time series plot of the wheat production data is shown below:\n\n\nCode\nstart_date = c(2002, 8)\nwheat_ts = ts(wheat_usa$Yield, start=start_date, frequency=12)\nautoplot(wheat_ts)+ylab(\"Wheat Production (Tonnes/Hectare)\")\n\n\n\n\n\nFrom the plot, we can see the general trend of the yield since 2002 is going up, from below 2.5 to around 3.1 tons per hectare. The data shows some degree of seasonality, as every once in a while the data repeats some patterns. Since there is not big up-and-downs shown in the plot, we can also infer that the yield data does not have much variations.\nNext, I will do a small decomposing step to further investigate the time series components.\n\n\nCode\ndecom_wheat = decompose(wheat_ts)\nautoplot(decom_wheat)+ggtitle(\"Decomposition of Wheat Production of USA\")+\n  labs(x = \"Time\")\n\n\n\n\n\nFrom the decomposition plots, though the trend is not smooth, it is still obvious that for most of the time it is going up. Next, we can see clear seasonality, as the pattern is repeating in the same way. This is expected since the wheat production heavily depends on the seasons and weathers in the year. From the last part of the plot, we can see that the remainder is large for data before 2005 and after 2020, meaning that during these time periods the data is fluctuating more than other times.\nNext, lag plots of the production data should be useful to investigate the randomness of the data.\n\n\nCode\nlag1 = gglagplot(wheat_ts, do.lines=FALSE, lags=4)+\n  xlab(\"Lags\")+ylab(\"Yi\")\nlag1\n\n\n\n\n\nFrom the lag plots, we can see very obvious linear pattern in lag 1 and lag 2 plots, and the pattern is weaker but still visible in lag3 and lag 4 plots. A linear pattern in the lag plot means that the time series data is strongly non-random, and also it may be appropriate to apply an auto-regressive model on the data.\nTherefore, it seems that the wheat data in USA is non random. Now, ACF and PACF plots will be useful to look at the auto correlation of the data.\n\n\nCode\nggPacf(wheat_ts) + ggtitle(\"ACF of Monthly Wheat Production in USA\")\n\n\n\n\n\nFrom the ACF plot, it seems that the ACF of the lags around 12 and 24 is higher, where the ACF of the lags around 6 and 18 is lower. This should be caused by the seasonal pattern, there each year the peaks and troughs tend to be 6 months apart. The dashed blue lines indicate that the correlations are a little apart from zero.\nIn conclusion, the ACF plot should indicate that the wheat production time series is stationary. I will now use an Augmented Dickey-Fuller Test (ADF test) to check this claim.\n\n\nCode\ntseries::adf.test(wheat_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  wheat_ts\nDickey-Fuller = -4.5741, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\nSince the p-value is 0.01, we can reject the null and confirm that the series is stationary. Therefore, the test supports the result from the ACF plot."
  },
  {
    "objectID": "eda.html#moving-average-smoothing",
    "href": "eda.html#moving-average-smoothing",
    "title": "Exploratory Data Analysis",
    "section": "Moving Average Smoothing",
    "text": "Moving Average Smoothing\nApart from the decomposing, lag plots and ACF plots in the previous steps, I will perform a moving average smoothing on the USA monthly yield data with a different crop (corn).\nFirst, read in the data, and replace the NA values. Again I choose to fix them with linear interpolation.\n\n\nCode\ncp = read.csv(\"./HW3/Monthly_Data.csv\")\n\ncorn_usa = cp[cp$Crop==\"Corn\",]\ncolnames(corn_usa)[4] = c(\"Yield\")\ncorn_usa$Yield = na.approx(corn_usa$Yield)\n\n\nHaving the cleaned data, first let’s look at the time series plot of the corn production data:\n\n\nCode\nstart_date = c(2002, 8)\ncorn_ts = ts(corn_usa$Yield, start=start_date, frequency=12)\nautoplot(corn_ts)+ylab(\"Corn Production (Tonnes/Hectare)\")\n\n\n\n\n\nTo help identifying the underlying patterns of this time series, I will use a SMA smoothing method with various appropriate MA windows.\nFirst, try a 5-MA on the monthly data:\n\n\nCode\nautoplot(corn_ts, series=\"Monthly Data\")+\n  autolayer(ma(corn_ts, 5), series=\"5-MA\")+\n  xlab(\"Month\")+ylab(\"Corn Yield\")+\n  scale_color_manual(values=c(\"Monthly Data\"=\"grey50\", \"5-MA\"=\"red\"), breaks=c(\"Monthly Data\", \"5-MA\"))\n\n\n\n\n\nSince this is a time series with relatively less data points, the 5-moving average on the original monthly data has limited effect with this small MA window, but it is still obvious that the red line is smoother than the original data. The good side of using a 5-MA is that the seasonality component in the time series is clearly visible.\nNext, I will try some larger numbers for the MA window.\n\n\nCode\nautoplot(corn_ts, series=\"Monthly Data\")+\n  autolayer(ma(corn_ts, 12), series=\"12-MA\")+\n  xlab(\"Month\")+ylab(\"Corn Yield\")+\n  scale_color_manual(values=c(\"Monthly Data\"=\"grey50\", \"12-MA\"=\"red\"), breaks=c(\"Monthly Data\", \"12-MA\"))\n\n\n\n\n\nA larger MA window of 12 created a smoother moving average line, however the seasonality is not that obvious now.\nThen, I want to focus on some other components of the time series like overall trend or the error of the data, so I will use a large window and compare the moving averages.\n\n\nCode\nautoplot(ma(corn_ts, 5), series=\"5-MA\")+\n  autolayer(ma(corn_ts, 53), series=\"53-MA\")+\n  xlab(\"Month\")+ylab(\"Corn Yield\")+\n  scale_color_manual(values=c(\"5-MA\"=\"grey50\", \"53-MA\"=\"red\"), breaks=c(\"5-MA\", \"53-MA\"))\n\n\n\n\n\nThe 53-MA line here apparently is much more concise than the previous lines, but now only trend component is shown. Still, we can see that overall the corn yield is increasing from 9 to 11 tonnes/hectare, and there is a decline during 2010 to 2011. The other information of the time series is not obvious for this moving average line."
  },
  {
    "objectID": "financial.html",
    "href": "financial.html",
    "title": "Financial Time Series Models",
    "section": "",
    "text": "For this tab, the financial time series models will be implemented, to mainly forecast the volatility of future returns, an important factor in crops trading. The data for this part of analysis is the closing returns of stocks of Bunge Limited, an American agribusiness company that mills, manufactures, and sells wheat as a main business (Source).\n\n\nCode\ngetSymbols(\"BG\", from=\"2012-01-01\", src=\"yahoo\")\n\n\n[1] \"BG\"\n\n\n\n\n\nFirst, let’s look at the plot of the data and check the stationarity and volatility.\n\n\nCode\nchartSeries(BG, theme=chartTheme(\"white\"), bar.type=\"hlc\", \n            up.col=\"green\", dn.col=\"red\")\n\n\n\n\n\n\n\nCode\nBG_close = Ad(BG)\nreturns = diff(log(BG_close))\nchartSeries(returns, theme=chartTheme(\"white\"), bar.type=\"hlc\", \n            up.col=\"green\", dn.col=\"red\")\n\n\n\n\n\nFirst, from the candlestick time series plot, there is a huge increase of the closing price of the stock since 2020, however it started to decline and is at a constant level since 2022. The closing returns does not look completely stationary, and the volatility is also hard to tell. After the differencing and log transform steps, the data seems to be stationary, while also having an increase in volatility.\n\n\n\n\nACF/PACF\n\n\nCode\nggAcf(returns)\n\n\n\n\n\nCode\nggPacf(returns)\n\n\n\n\n\nThen I will look at the ACF and PACF of the absolute returns and squared returns.\n\n\nCode\nggAcf(abs(returns))\n\n\n\n\n\nCode\nggAcf(returns**2)\n\n\n\n\n\nIt seems that there is high correlation for the absolute and square of the returns. Therefore, fitting with just ARCH/GARCH model is not enough, and AR/ARIMA+ARCH/GARCH should be used.\n\n\n\n\n\nSelect AR/ARMA/ARIMA Model\nFirst, with AR model I want to choose the best p value.\n\n\nCode\nscores = matrix(rep(NA,4*6), nrow=6, ncol=4)\n\ni=1\nfor (p in 0:5){\n  model = Arima(returns, order=c(p,0,0), include.drift=TRUE)\n  scores[i,] = c(p, model$aic, model$bic, model$aicc)\n  i = i+1\n}\nscores = as.data.frame(scores)\ncolnames(scores) = c(\"p\",\"AIC\",\"BIC\",\"AICc\")\nscores\n\n\n  p       AIC       BIC      AICc\n1 0 -14697.77 -14679.89 -14697.76\n2 1 -14718.70 -14694.88 -14718.69\n3 2 -14717.92 -14688.13 -14717.89\n4 3 -14716.69 -14680.95 -14716.66\n5 4 -14716.81 -14675.12 -14716.77\n6 5 -14717.24 -14669.58 -14717.19\n\n\nIt is obvious that p=1 is the best model.\n\n\nWith ARIMA model I want to choose the best p,d,q values.\n\n\nCode\nscores = matrix(rep(0, 6*18), nrow=18, ncol=6)\n\ni=1\nfor (p in 0:2){\n  for (d in 0:1){\n    for (q in 0:2){\n      model = Arima(returns, order=c(p,d,q), include.drift=TRUE)\n      scores[i,] = c(p,d,q,model$aic, model$bic, model$aicc)\n      i = i+1\n    }\n  }\n}\n\nscores = as.data.frame(scores)\ncolnames(scores) = c(\"p\",\"d\", \"q\",\"AIC\",\"BIC\",\"AICC\")\nknitr::kable(scores, format=\"pipe\", padding=30, digits=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICC\n\n\n\n\n0\n0\n0\n-14697.77\n-14679.89\n-14697.76\n\n\n0\n0\n1\n-14717.69\n-14693.86\n-14717.68\n\n\n0\n0\n2\n-14717.46\n-14687.68\n-14717.44\n\n\n0\n1\n0\n-12470.96\n-12459.05\n-12470.96\n\n\n0\n1\n1\n-14683.66\n-14665.79\n-14683.65\n\n\n0\n1\n2\n-14703.40\n-14679.57\n-14703.38\n\n\n1\n0\n0\n-14718.70\n-14694.88\n-14718.69\n\n\n1\n0\n1\n-14718.55\n-14688.77\n-14718.53\n\n\n1\n0\n2\n-14716.61\n-14680.87\n-14716.58\n\n\n1\n1\n0\n-13515.42\n-13497.55\n-13515.41\n\n\n1\n1\n1\n-14704.42\n-14680.59\n-14704.41\n\n\n1\n1\n2\n-14704.31\n-14674.53\n-14704.29\n\n\n2\n0\n0\n-14717.92\n-14688.13\n-14717.89\n\n\n2\n0\n1\n-14715.84\n-14680.10\n-14715.81\n\n\n2\n0\n2\n-14714.64\n-14672.95\n-14714.60\n\n\n2\n1\n0\n-13847.40\n-13823.57\n-13847.38\n\n\n2\n1\n1\n-14703.67\n-14673.89\n-14703.65\n\n\n2\n1\n2\n-14702.34\n-14666.60\n-14702.31\n\n\n\n\n\nIt seems that the model ARIMA(1,0,0) is the best model here, and this is the same model as the previous step. Therefore, it can be determined that the best model for now is AR(1) model.\n\n\n\n\n\n\nStandardized Residuals Plot\nI will now check the standardized residuals plot of the previous selected model, to see whether further modeling is needed.\n\n\nCode\nar1 = Arima(returns, order=c(1,0,0))\nres1 = ar1$residuals\nggAcf(res1^2)\n\n\n\n\n\nCode\nggPacf(res1^2)\n\n\n\n\n\nIt seems that there is still high correlation from the ACF and PACF of the squared residuals, therefore for further modeling I can look for appropriate ARCH or GARCH model and compare the performances.\n\n\n\n\n\nAR+ARCH Model\nIn this step I will fit the AR model with different ARCH models and find a best one.\n\n\nCode\narch_list = list()\ni=1\nfor (p in 1:10){\n  arch_list[[i]] = garch(ar1$res[2:(length(ar1$res)-1)], order=c(0,p), trace=F)\n  i = i+1\n}\n\n\nLook for the best ARCH model here by comparing the AIC scores.\n\n\nCode\naic_arch = sapply(arch_list, AIC)\nwhich(aic_arch==min(aic_arch))\n\n\n[1] 3\n\n\nTherefore, ARCH3 is the best one here.\n\n\nCode\narch3 = garch(ar1$res[2:(length(ar1$res)-1)], order=c(0,3), trace=F)\nprint(aic_arch[3])\n\n\n[1] -15088.7\n\n\n\n\n\n\n\nAR+GARCH Model\nFor the AR+GARCH model, based on the ACF and PACF of the squared residuals of AR(1), I picked the orders of the model to be order[1]=[1,2], order[2]=[1,2].\n\n\nCode\ngarch11 = garchFit(formula=~garch(1,1), data=res1[2:(length(res1)-1)], trace=F)\ngarch12 = garchFit(formula=~garch(1,2), data=res1[2:(length(res1)-1)], trace=F)\ngarch21 = garchFit(formula=~garch(2,1), data=res1[2:(length(res1)-1)], trace=F)\ngarch22 = garchFit(formula=~garch(2,2), data=res1[2:(length(res1)-1)], trace=F)\n\n\n\n\nCode\ngarch_models = c(\"garch(1,1)\", \"garch(1,2)\", \"garch(2,1)\", \"garch(2,2)\")\ngarch_AIC = c(garch11@fit$ics[1],garch12@fit$ics[1],garch21@fit$ics[1], garch22@fit$ics[1])\ngarch_BIC = c(garch11@fit$ics[2],garch12@fit$ics[2],garch21@fit$ics[2], garch22@fit$ics[2])\ngarch_SIC = c(garch11@fit$ics[3],garch12@fit$ics[3],garch21@fit$ics[3], garch22@fit$ics[3])\n\ngarchs = data.frame(garch_models, garch_AIC, garch_BIC, garch_SIC)\ngarchs\n\n\n  garch_models garch_AIC garch_BIC garch_SIC\n1   garch(1,1) -5.302291 -5.293943 -5.302295\n2   garch(1,2) -5.304602 -5.294167 -5.304609\n3   garch(2,1) -5.301434 -5.290998 -5.301440\n4   garch(2,2) -5.303902 -5.291380 -5.303911\n\n\nIt looks like that GARCH(1,2) is the best model based on the three measures. Therefore, the best model here is AR(1)+GARCH(1,2).\n\n\n\n\n\nBest Model\nTo find the better one from AR(1)+ARCH(3) and AR(1)+GARCH(1,2), I will compare their AIC score.\n\n\nCode\nprint(paste0(\"AIC of ARCH(3) = \", aic_arch[3]))\n\n\n[1] \"AIC of ARCH(3) = -15088.6996502879\"\n\n\nCode\nprint(paste0(\"AIC of GARCH(1,2) = \", garch11@fit$ics[1]*length(garch12@data)))\n\n\n[1] \"AIC of GARCH(1,2) = -15132.7394635695\"\n\n\nIt looks like the AIC of the AR(1)+GARCH(1,2) model is smaller. We will then look at some model diagnosis of these two models.\n\n\nCode\nsummary(arch3)\n\n\n\nCall:\ngarch(x = ar1$res[2:(length(ar1$res) - 1)], order = c(0, 3),     trace = F)\n\nModel:\nGARCH(0,3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-8.75236 -0.47008  0.02229  0.51018 10.54720 \n\nCoefficient(s):\n    Estimate  Std. Error  t value Pr(>|t|)    \na0 2.002e-04   2.675e-06   74.836  < 2e-16 ***\na1 2.106e-01   1.708e-02   12.329  < 2e-16 ***\na2 1.606e-01   1.464e-02   10.973  < 2e-16 ***\na3 6.456e-02   9.958e-03    6.484 8.95e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nDiagnostic Tests:\n    Jarque Bera Test\n\ndata:  Residuals\nX-squared = 20717, df = 2, p-value < 2.2e-16\n\n\n    Box-Ljung test\n\ndata:  Squared.Residuals\nX-squared = 0.27658, df = 1, p-value = 0.599\n\n\n\n\nCode\nsummary(garch12)\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 2), data = res1[2:(length(res1) - \n    1)], trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 2)\n<environment: 0x0000011bb7955e68>\n [data = res1[2:(length(res1) - 1)]]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n        mu       omega      alpha1       beta1       beta2  \n4.0753e-05  3.1451e-05  1.1964e-01  1.8923e-01  5.9916e-01  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)    \nmu     4.075e-05   3.032e-04    0.134   0.8931    \nomega  3.145e-05   4.814e-06    6.533 6.46e-11 ***\nalpha1 1.196e-01   1.649e-02    7.254 4.06e-13 ***\nbeta1  1.892e-01   9.046e-02    2.092   0.0364 *  \nbeta2  5.992e-01   8.795e-02    6.812 9.60e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 7574.668    normalized:  2.654053 \n\nDescription:\n Wed May 10 00:37:46 2023 by user: 49789 \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value  \n Jarque-Bera Test   R    Chi^2  21465.62  0        \n Shapiro-Wilk Test  R    W      0.8911886 0        \n Ljung-Box Test     R    Q(10)  10.37341  0.4083662\n Ljung-Box Test     R    Q(15)  13.42768  0.5693006\n Ljung-Box Test     R    Q(20)  16.11302  0.7095885\n Ljung-Box Test     R^2  Q(10)  2.818155  0.9853854\n Ljung-Box Test     R^2  Q(15)  18.04306  0.2604083\n Ljung-Box Test     R^2  Q(20)  19.88941  0.4648663\n LM Arch Test       R    TR^2   3.854605  0.9859193\n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n-5.304602 -5.294167 -5.304609 -5.300839 \n\n\nLooking at the Ljung-Box test, the p-values of the tests on squared residuals are all greater than 0.05, with GARCH(1,2) has a little larger p-value, meaning that the two models both represents the residuals well. Therefore, AR(1)+GARCH(1,2) wins on lower AIC score and is the best model.\nThe equation of this model will be:\n\\[\nσ_t^2 = κ+γ_1σ_{t-1}^2+α_1ε_{t−1}^2+α_2ε_{t−2}^2\\]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Time Series",
    "section": "",
    "text": "What is a Time Series ?\n\nAny metric that is measured over regular time intervals makes a Time Series.\n\nExample: Weather data, Stock prices, Industry forecasts, etc are some of the common ones.\n\nThe analysis of experimental data that have been observed at different points in time leads to new and unique problems in statistical modeling and inference.\nThe obvious correlation introduced by the sampling of adjacent points in time can severely restrict the applicability of the many conventional statistical methods traditionally dependent on the assumption that these adjacent observations are independent and identically distributed.\n\nHere are some examples of time series, including stocks and climate.\n\n\n\n\n\nProject Topic:\nThe project will study time series of the production, selling price and stocks of crops, and the main focus will be on the analyzing and predicting yield and price of wheat in the US."
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Crop Yield\n\n\nThis project will study on world wide yield and its predictions, and then focus on the yield and price of wheat in the US. Yield is a measurement of the amount of crops grown and produced, including rice, wheat, beans, or even meat or milk. Each year, yields are determined by many factors, including locations or countries, crops types, land types, and climates. Many innovations and new techniques are making changes or improvements to yields, such as fertilizer, farming tools, crop varieties, etc. The yield of the crops then lead to other aspect of agriculture, including the amount of crops used for food, and the export and import quantities, as well as the selling price of the crops which is closely related to everyone.\nFood and crops are so important to us as it feeds our basic needs, and thus yields and price are necessary to study on, especially on the prediction of these variables. It not only enables farmers to see what their productions will be before harvest dates, but it also forecasts the food situation in the future. Specifically, this project will use time series models on the data of wheat in the US to perform yield and price predictions.\n\n\n\nWhat are the general situations of the yields data of different crops (trends, seasonality etc.)?\nHow do the yields data of each type of crops differ among the countries?\nWhat are the situations of the time series components (trends, seasonality etc.) of the yield of wheat in the US?\nForecast the wheat production in the US base on the previous data.\nWhat are the performances of the models with the prediction of the yield?\nWhat are the situations of the time series components (trends, seasonality etc.) of the price of wheat in the US?\nWhat factors have impact on the price of the wheat?\nHow do these factors affect the price of the wheat?\nForecast the wheat price in the US base on the previous data and other factors.\nWhat is the situation of the stocks related to wheat in the US? What is the best model to study on the stocks?"
  }
]