---
title: "Financial Time Series Models"
---

```{r, echo=FALSE,message=FALSE,warning=FALSE}
library(tidyverse)
library(forecast)
library(astsa) 
library(xts)
library(tseries)
library(fpp2)
library(fma)
library(lubridate)
library(TSstudio)
library(quantmod)
library(tidyquant)
library(plotly)
library(ggplot2)
library(gridExtra)
library(fGarch)
```

For this tab, the financial time series models will be implemented, to mainly forecast the volatility of future returns, an important factor in crops trading. The data for this part of analysis is the closing returns of stocks of Bunge Limited, an American agribusiness company that mills, manufactures, and sells wheat as a main business ([Source](https://ca.finance.yahoo.com/news/3-top-wheat-stocks-buy-090000987.html)).

```{r}
getSymbols("BG", from="2012-01-01", src="yahoo")
```

```{r, echo=FALSE,message=FALSE,warning=FALSE}
# wheat = read.csv("./HW4/USA_Wheat.csv")
# wheat = wheat[,c(1,3,4,5,6,7)]
# wheat$Yield = na.approx(wheat$Yield)
# wheat$Food_Use = as.numeric(gsub(",", "", wheat$Food_Use))
# wheat$Import = as.numeric(gsub(",", "", wheat$Import))
# wheat$Export = as.numeric(gsub(",", "", wheat$Export))
```

First, let's look at the plot of the data and check the stationarity and volatility.

```{r, echo=TRUE,message=FALSE,warning=FALSE}
chartSeries(BG, theme=chartTheme("white"), bar.type="hlc", 
            up.col="green", dn.col="red")
```

```{r, echo=TRUE,message=FALSE,warning=FALSE}
BG_close = Ad(BG)
returns = diff(log(BG_close))
chartSeries(returns, theme=chartTheme("white"), bar.type="hlc", 
            up.col="green", dn.col="red")
```

First, from the candlestick time series plot, there is a huge increase of the closing price of the stock since 2020, however it started to decline and is at a constant level since 2022. The closing returns does not look completely stationary, and the volatility is also hard to tell. After the differencing and log transform steps, the data seems to be stationary, while also having an increase in volatility.

#### 

### ACF/PACF

```{r, echo=TRUE,message=FALSE,warning=FALSE}
ggAcf(returns)
ggPacf(returns)
```

Then I will look at the ACF and PACF of the absolute returns and squared returns.

```{r, echo=TRUE,message=FALSE,warning=FALSE}
ggAcf(abs(returns))
ggAcf(returns**2)
```

It seems that there is high correlation for the absolute and square of the returns. Therefore, fitting with just ARCH/GARCH model is not enough, and AR/ARIMA+ARCH/GARCH should be used.

#### 

### Select AR/ARMA/ARIMA Model

First, with AR model I want to choose the best p value.

```{r, echo=TRUE,message=FALSE,warning=FALSE}
scores = matrix(rep(NA,4*6), nrow=6, ncol=4)

i=1
for (p in 0:5){
  model = Arima(returns, order=c(p,0,0), include.drift=TRUE)
  scores[i,] = c(p, model$aic, model$bic, model$aicc)
  i = i+1
}
scores = as.data.frame(scores)
colnames(scores) = c("p","AIC","BIC","AICc")
scores
```

It is obvious that p=1 is the best model.

#### 

With ARIMA model I want to choose the best p,d,q values.

```{r, echo=TRUE,message=FALSE,warning=FALSE}
scores = matrix(rep(0, 6*18), nrow=18, ncol=6)

i=1
for (p in 0:2){
  for (d in 0:1){
    for (q in 0:2){
      model = Arima(returns, order=c(p,d,q), include.drift=TRUE)
      scores[i,] = c(p,d,q,model$aic, model$bic, model$aicc)
      i = i+1
    }
  }
}

scores = as.data.frame(scores)
colnames(scores) = c("p","d", "q","AIC","BIC","AICC")
knitr::kable(scores, format="pipe", padding=30, digits=2)
```

It seems that the model ARIMA(1,0,0) is the best model here, and this is the same model as the previous step. Therefore, it can be determined that the best model for now is AR(1) model.

#### 

### Standardized Residuals Plot

I will now check the standardized residuals plot of the previous selected model, to see whether further modeling is needed.

```{r, echo=TRUE,message=FALSE,warning=FALSE}
ar1 = Arima(returns, order=c(1,0,0))
res1 = ar1$residuals
ggAcf(res1^2)
ggPacf(res1^2)
```

It seems that there is still high correlation from the ACF and PACF of the squared residuals, therefore for further modeling I can look for appropriate ARCH or GARCH model and compare the performances.

#### 

### AR+ARCH Model

In this step I will fit the AR model with different ARCH models and find a best one.

```{r, echo=TRUE,message=FALSE,warning=FALSE}
arch_list = list()
i=1
for (p in 1:10){
  arch_list[[i]] = garch(ar1$res[2:(length(ar1$res)-1)], order=c(0,p), trace=F)
  i = i+1
}
```

Look for the best ARCH model here by comparing the AIC scores.

```{r, echo=TRUE,message=FALSE,warning=FALSE}
aic_arch = sapply(arch_list, AIC)
which(aic_arch==min(aic_arch))
```

Therefore, ARCH3 is the best one here.

```{r, echo=TRUE,message=FALSE,warning=FALSE}
arch3 = garch(ar1$res[2:(length(ar1$res)-1)], order=c(0,3), trace=F)
print(aic_arch[3])
```

#### 

### AR+GARCH Model

For the AR+GARCH model, based on the ACF and PACF of the squared residuals of AR(1), I picked the orders of the model to be order\[1\]=\[1,2\], order\[2\]=\[1,2\].

```{r, echo=TRUE,message=FALSE,warning=FALSE}
garch11 = garchFit(formula=~garch(1,1), data=res1[2:(length(res1)-1)], trace=F)
garch12 = garchFit(formula=~garch(1,2), data=res1[2:(length(res1)-1)], trace=F)
garch21 = garchFit(formula=~garch(2,1), data=res1[2:(length(res1)-1)], trace=F)
garch22 = garchFit(formula=~garch(2,2), data=res1[2:(length(res1)-1)], trace=F)
```

```{r, echo=TRUE,message=FALSE,warning=FALSE}
garch_models = c("garch(1,1)", "garch(1,2)", "garch(2,1)", "garch(2,2)")
garch_AIC = c(garch11@fit$ics[1],garch12@fit$ics[1],garch21@fit$ics[1], garch22@fit$ics[1])
garch_BIC = c(garch11@fit$ics[2],garch12@fit$ics[2],garch21@fit$ics[2], garch22@fit$ics[2])
garch_SIC = c(garch11@fit$ics[3],garch12@fit$ics[3],garch21@fit$ics[3], garch22@fit$ics[3])

garchs = data.frame(garch_models, garch_AIC, garch_BIC, garch_SIC)
garchs
```

It looks like that GARCH(1,2) is the best model based on the three measures. Therefore, the best model here is AR(1)+GARCH(1,2).

#### 

### Best Model

To find the better one from AR(1)+ARCH(3) and AR(1)+GARCH(1,2), I will compare their AIC score.

```{r, echo=TRUE,message=FALSE,warning=FALSE}
print(paste0("AIC of ARCH(3) = ", aic_arch[3]))
print(paste0("AIC of GARCH(1,2) = ", garch11@fit$ics[1]*length(garch12@data)))
```

It looks like the AIC of the AR(1)+GARCH(1,2) model is smaller. We will then look at some model diagnosis of these two models.

```{r, echo=TRUE,message=FALSE,warning=FALSE}
summary(arch3)
```

```{r, echo=TRUE,message=FALSE,warning=FALSE}
summary(garch12)
```

Looking at the Ljung-Box test, the p-values of the tests on squared residuals are all greater than 0.05, with GARCH(1,2) has a little larger p-value, meaning that the two models both represents the residuals well. Therefore, AR(1)+GARCH(1,2) wins on lower AIC score and is the best model.

The equation of this model will be:

$$
σ_t^2 = κ+γ_1σ_{t-1}^2+α_1ε_{t−1}^2+α_2ε_{t−2}^2$$
